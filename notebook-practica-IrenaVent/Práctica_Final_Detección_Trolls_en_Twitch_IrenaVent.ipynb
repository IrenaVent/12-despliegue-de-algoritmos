{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Práctica_Final_Detección_Trolls_en_Twitch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"Cj8bcxFX85UO"},"source":["# Práctica Final: detección de mensajes troll en chat de Twitch en tiempo real\n","\n","Durante este último año la plataforma de vídeo en streaming Twitch ha cogido mucha popularidad debido a la situación que hemos vivido debido al COVID-19. Por esto, mucha gente de todas las edades ha empezado a consumir esta plataforma de manera diaria.\n","\n","Como consecuencia, no sólo han aumentado las personas que ven contenido en Twitch, sino también el número de los denominados *trolls*, gente que pone comentarios ofensivos en los chat de los streamers.\n","\n","En esta práctica se desarrollará un sistema autónomo basado en IA y desplegado en GCP que detectará en tiempo real si los mensajes que se envían a un canal de Twitch son de un *troll* o no. La práctica constará de tres partes principales que serán evaluadas en la corrección:\n","1. Entrenamiento e inferencia en Batch de un modelo usando Dataflow y AI Platform. **(3.5 puntos)**.\n","2. Despliegue e inferencia online en microservicio con el modelo. **(3.5 puntos)**.\n","3. Inferencia en streaming de un canal de Twitch con el microservicio anterior. **(3 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"hgE9XB8ACVmQ"},"source":["# Configuración de nuestro proyecto en GCP\n"]},{"cell_type":"code","metadata":{"id":"zAorVw7RCT9C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443711296,"user_tz":-120,"elapsed":2584,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"f5207790-e0f4-4383-d7b3-c2ec8110ea95"},"source":["PROJECT_ID = \"twitch-practice-355807\" #@param {type:\"string\"}\n","! gcloud config set project $PROJECT_ID"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n"]}]},{"cell_type":"code","metadata":{"id":"hbAl8pSkCXCm","executionInfo":{"status":"ok","timestamp":1657443711698,"user_tz":-120,"elapsed":407,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"source":["import sys\n","\n","# If you are running this notebook in Colab, run this cell and follow the\n","# instructions to authenticate your GCP account. This provides access to your\n","# Cloud Storage bucket and lets you submit training jobs and prediction\n","# requests.\n","\n","if 'google.colab' in sys.modules:\n","  from google.colab import auth as google_auth\n","  google_auth.authenticate_user()\n","\n","# If you are running this notebook locally, replace the string below with the\n","# path to your service account key and run this cell to authenticate your GCP\n","# account.\n","else:\n","  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6Zpx14FCfXy","executionInfo":{"status":"ok","timestamp":1657443711699,"user_tz":-120,"elapsed":3,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"source":["BUCKET_NAME = \"twitch-practice-355807-irena-vent\" #@param {type:\"string\"}\n","REGION = \"europe-west1\" #@param {type:\"string\"}"],"execution_count":3,"outputs":[]},{"cell_type":"code","source":["! gsutil mb -l $REGION gs://$BUCKET_NAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFlqQpF7DPHm","executionInfo":{"status":"ok","timestamp":1657380360686,"user_tz":-120,"elapsed":4010,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"2f8804ba-e776-47f8-9d65-921cd989a271"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating gs://twitch-practice-355807-irena-vent/...\n","ServiceException: 409 A Cloud Storage bucket named 'twitch-practice-355807-irena-vent' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"]}]},{"cell_type":"code","metadata":{"id":"lE4V-Xt0ChSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443715872,"user_tz":-120,"elapsed":4176,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"285520d2-eb56-46c9-935a-29e4d56bb1f1"},"source":["! gsutil ls -al gs://$BUCKET_NAME"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["   2756023  2022-07-09T08:33:49Z  gs://twitch-practice-355807-irena-vent/data.json#1657355629890691  metageneration=1\n","                                 gs://twitch-practice-355807-irena-vent/beam-temp/\n","                                 gs://twitch-practice-355807-irena-vent/model/\n","                                 gs://twitch-practice-355807-irena-vent/predictions/\n","                                 gs://twitch-practice-355807-irena-vent/trainer/\n","                                 gs://twitch-practice-355807-irena-vent/transformed_data/\n","TOTAL: 1 objects, 2756023 bytes (2.63 MiB)\n"]}]},{"cell_type":"markdown","metadata":{"id":"3D8Qw14y_nRa"},"source":["# Entrenamiento e inferencia en Batch\n","\n","Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**.\n","\n","A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n","\n","![batch_diagram](https://drive.google.com/uc?export=view&id=1h1BkIunyKSkJYFRbXKNWpHOZ_rDUyGAT)"]},{"cell_type":"markdown","metadata":{"id":"qM_dh-471gIq"},"source":["A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región para evitar problemas más adelante."]},{"cell_type":"code","metadata":{"id":"Sqp4L_nmUjAc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657355630535,"user_tz":-120,"elapsed":5593,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"6faa3a2b-2761-4800-e614-64539312b19d"},"source":["# Upload data to your bucket\n","! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json -O - | gsutil cp - gs://$BUCKET_NAME/data.json"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-09 08:33:44--  https://storage.googleapis.com/twitch-practice-keepcoding/data.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.107.128, 173.194.202.128, 108.177.98.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.107.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2756023 (2.6M) [application/json]\n","Saving to: ‘STDOUT’\n","\n","-                     0%[                    ]       0  --.-KB/s               Copying from <STDIN>...\n","-                   100%[===================>]   2.63M   944KB/s    in 2.9s    \n","\n","2022-07-09 08:33:47 (944 KB/s) - written to stdout [2756023/2756023]\n","\n","/ [1 files][    0.0 B/    0.0 B]                                                \n","Operation completed over 1 objects.                                              \n"]}]},{"cell_type":"markdown","metadata":{"id":"vCQQ9j2I11tg"},"source":["Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."]},{"cell_type":"code","metadata":{"id":"PsblBlJ6RrGm"},"source":["%mkdir /content/batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyK51quU1_oi"},"source":["Se establece el directorio de trabajo que hemos creado."]},{"cell_type":"code","metadata":{"id":"s7ybSaotRwkP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657361437010,"user_tz":-120,"elapsed":375,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"77df7330-fd7e-434f-ed25-eac769b5c345"},"source":["import os\n","\n","# Set the working directory to the sample code directory\n","%cd /content/batch\n","\n","WORK_DIR = os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/batch\n"]}]},{"cell_type":"markdown","metadata":{"id":"4NSd4bAo2aj_"},"source":["Ahora se descargarán los datos en el workspace de Colab para trabajar en local."]},{"cell_type":"code","metadata":{"id":"qOmUJSg1JCgg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657355665691,"user_tz":-120,"elapsed":242,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"d05e7cf2-a6db-4338-d98b-d5cddcc1ea4d"},"source":["! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-09 08:34:25--  https://storage.googleapis.com/twitch-practice-keepcoding/data.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.142.128, 2607:f8b0:400e:c0c::80, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2756023 (2.6M) [application/json]\n","Saving to: ‘data.json’\n","\n","\rdata.json             0%[                    ]       0  --.-KB/s               \rdata.json           100%[===================>]   2.63M  --.-KB/s    in 0.01s   \n","\n","2022-07-09 08:34:25 (253 MB/s) - ‘data.json’ saved [2756023/2756023]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"eRoW6zyg2kEz"},"source":["Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."]},{"cell_type":"code","metadata":{"id":"9s0-8_JK_z2D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657368563353,"user_tz":-120,"elapsed":327,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"bab02a27-2dfa-4df6-adb5-166ad62ca1e7"},"source":["%%writefile requirements.txt\n","\n","apache-beam[gcp]==2.24.0\n","tensorflow==2.8.0\n","gensim==3.6.0\n","fsspec==0.8.4\n","gcsfs==0.7.1\n","numpy==1.20.0\n","pandas==1.3.5\n","keras==2.8.0 "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting requirements.txt\n"]}]},{"cell_type":"markdown","metadata":{"id":"yIl8lrPp2x2j"},"source":["Instalamos las dependencias. **No olvidarse de reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"]},{"cell_type":"code","metadata":{"id":"ZS6P8A4c_8le","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657355965089,"user_tz":-120,"elapsed":3935,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"edb41dfe-80f5-4b81-8b34-44c8eaaec914"},"source":["! pip install -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: apache-beam[gcp]==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.24.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.8.2+zzzcolab20220527125636)\n","Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.6.0)\n","Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.8.4)\n","Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.7.1)\n","Requirement already satisfied: numpy==1.20.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.20.0)\n","Requirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.3.5)\n","Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.8.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (1.4.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (4.4.2)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (0.4.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (3.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (2.28.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5->-r requirements.txt (line 8)) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5->-r requirements.txt (line 8)) (2022.1)\n","Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.7.0)\n","Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.7.4.3)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.12.3)\n","Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.0.0)\n","Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.3.1.1)\n","Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.9.2.1)\n","Requirement already satisfied: fastavro<0.24,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.23.6)\n","Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.17.3)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7)\n","Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.0)\n","Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.18.2)\n","Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.46.3)\n","Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.4)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: pyarrow<0.18.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.1)\n","Requirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.8.0)\n","Requirement already satisfied: cachetools<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.1.1)\n","Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.2)\n","Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.2)\n","Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.2.2)\n","Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.16.3)\n","Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.2)\n","Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.2)\n","Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.3)\n","Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.21.0)\n","Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.2)\n","Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.5.31)\n","Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.2)\n","Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.3)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (57.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (4.8)\n","Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.1)\n","Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.12.4)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.31.6)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (21.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.56.2)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.6.2)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.7/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (5.9.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.8)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2022.6.15)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.24.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.1.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.14.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.2)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.5.3)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.8.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.26.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (14.0.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 3)) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements.txt (line 3)) (1.5.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (3.3.7)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (3.8.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 6)) (3.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (21.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.7.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.3.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (4.0.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"k0DDu27S3CJv"},"source":["##**Entreglable (0.5 puntos)**\n","\n","Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n","\n","- Proporcionar dos modos de ejecución: `train` y `test`\n","- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."]},{"cell_type":"code","metadata":{"id":"1blctlxs_dPO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657360302474,"user_tz":-120,"elapsed":248,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"9aa3be07-ede9-4e13-e7f4-0bb574412a99"},"source":["%%writefile preprocess.py\n","\n","from __future__ import absolute_import\n","\n","import argparse\n","import logging\n","import re\n","import os\n","import csv\n","import json\n","import random\n","\n","from past.builtins import unicode\n","\n","import apache_beam as beam\n","from apache_beam.io import ReadFromText\n","from apache_beam.io import WriteToText\n","from apache_beam.coders.coders import Coder\n","from apache_beam.options.pipeline_options import PipelineOptions\n","from apache_beam.options.pipeline_options import SetupOptions, DirectOptions\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","\n","nltk.download(\"stopwords\")\n","\n","# CLEANING\n","STOP_WORDS = stopwords.words(\"english\")\n","STEMMER = SnowballStemmer(\"english\")\n","TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n","\n","\n","class ExtractColumnsDoFn(beam.DoFn):\n","    def process(self, element):\n","        # space removal\n","        element_split = json.loads(element)\n","        yield element_split['content'], element_split['annotation']['label'][0]\n","\n","\n","class PreprocessColumnsTrainFn(beam.DoFn):\n","    def process_iftroll(self, iftroll):\n","        iftroll = int(iftroll)\n","        if iftroll == 1:\n","            return \"TROLL\"\n","        else:\n","            return \"NOTROLL\"\n","\n","    def process_text(self, text):\n","        # Remove link,user and special characters\n","        stem = False\n","        text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip()\n","        tokens = []\n","        for token in text.split():\n","            if token not in STOP_WORDS:\n","                if stem:\n","                    tokens.append(STEMMER.stem(token))\n","                else:\n","                    tokens.append(token)\n","        return \" \".join(tokens)\n","\n","    def process(self, element):\n","        processed_text = self.process_text(element[0])\n","        processed_iftroll = self.process_iftroll(element[1])\n","        yield f\"{processed_text}, {processed_iftroll}\"\n","\n","\n","class CustomCoder(Coder):\n","    \"\"\"A custom coder used for reading and writing strings\"\"\"\n","\n","    def __init__(self, encoding: str):\n","        # latin-1\n","        # iso-8859-1\n","        self.enconding = encoding\n","\n","    def encode(self, value):\n","        return value.encode(self.enconding)\n","\n","    def decode(self, value):\n","        return value.decode(self.enconding)\n","\n","    def is_deterministic(self):\n","        return True\n","\n","\n","def run(argv=None, save_main_session=True):\n","\n","    \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n","\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument(\n","        \"--work-dir\", dest=\"work_dir\", required=True, help=\"Working directory\",\n","    )\n","\n","    parser.add_argument(\n","        \"--input\", dest=\"input\", required=True, help=\"Input dataset in work dir\",\n","    )\n","    parser.add_argument(\n","        \"--output\",\n","        dest=\"output\",\n","        required=True,\n","        help=\"Output path to store transformed data in work dir\",\n","    )\n","    parser.add_argument(\n","        \"--mode\",\n","        dest=\"mode\",\n","        required=True,\n","        choices=[\"train\", \"test\"],\n","        help=\"Type of output to store transformed data\",\n","    )\n","\n","    known_args, pipeline_args = parser.parse_known_args(argv)\n","\n","    # We use the save_main_session option because one or more DoFn's in this\n","    # workflow rely on global context (e.g., a module imported at module level).\n","    pipeline_options = PipelineOptions(pipeline_args)\n","    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n","    pipeline_options.view_as(DirectOptions).direct_num_workers = 0\n","\n","    # The pipeline will be run on exiting the with block.\n","    with beam.Pipeline(options=pipeline_options) as p:\n","\n","        # Read the text file[pattern] into a PCollection.\n","        raw_data = p | \"ReadTwitterData\" >> ReadFromText(\n","            known_args.input, coder=CustomCoder(\"latin-1\")\n","        )\n","\n","        if known_args.mode == \"train\":\n","\n","            transformed_data = (\n","                raw_data\n","                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n","                | \"Preprocess\" >> beam.ParDo(PreprocessColumnsTrainFn())\n","            )\n","\n","            eval_percent = 20\n","            assert 0 < eval_percent < 100, \"eval_percent must in the range (0-100)\"\n","            train_dataset, eval_dataset = (\n","                transformed_data\n","                | \"Split dataset\"\n","                >> beam.Partition(\n","                    lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2\n","                )\n","            )\n","\n","            train_dataset | \"TrainWriteToCSV\" >> WriteToText(\n","                os.path.join(known_args.output, \"train\", \"part\")\n","            )\n","            eval_dataset | \"EvalWriteToCSV\" >> WriteToText(\n","                os.path.join(known_args.output, \"eval\", \"part\")\n","            )\n","\n","        else:\n","            transformed_data = (\n","                raw_data\n","                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n","                | \"Preprocess\" >> beam.Map(lambda x: f'\"{x[0]}\"')\n","            )\n","\n","            transformed_data | \"TestWriteToCSV\" >> WriteToText(\n","                os.path.join(known_args.output, \"test\", \"part\")\n","            )\n","\n","\n","if __name__ == \"__main__\":\n","    logging.getLogger().setLevel(logging.INFO)\n","    run()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting preprocess.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"qawuKC0k4B0e"},"source":["Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"]},{"cell_type":"code","metadata":{"id":"n1MQvWsk_mVX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657360318163,"user_tz":-120,"elapsed":230,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"be7f14b9-66da-4b52-80a0-da73e966f961"},"source":["%%writefile setup.py\n","\n","import setuptools\n","\n","REQUIRED_PACKAGES = [\n","  \"apache-beam[gcp]==2.24.0\",\n","  \"tensorflow==2.8.0\",\n","  \"gensim==3.6.0\",\n","  \"fsspec==0.8.4\",\n","  \"gcsfs==0.7.1\",\n","  \"numpy==1.20.0\",\n","  \"keras==2.8.0\",\n","]\n","\n","setuptools.setup(\n","    name=\"twitchstreaming\",\n","    version=\"0.0.1\",\n","    install_requires=REQUIRED_PACKAGES,\n","    packages=setuptools.find_packages(),\n","    include_package_data=True,\n","    description=\"Troll detection\",\n",")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting setup.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"jLDlz_fL4UJA"},"source":["### Validación preprocess train en local (0.25 puntos)\n","\n","Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."]},{"cell_type":"code","metadata":{"id":"vFpirg37C3bN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657360325499,"user_tz":-120,"elapsed":4760,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"6b6d7752-d31a-4c87-8c32-91381efd9813"},"source":["! python3 preprocess.py \\\n","  --work-dir $WORK_DIR \\\n","  --runner DirectRunner \\\n","  --input $WORK_DIR/data.json \\\n","  --output $WORK_DIR/transformed_data \\\n","  --mode train"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f18b264fb90> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f18b264fcb0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f18b264fd40> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f18b264fdd0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f18b264fe60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f18b264ff80> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f18b264d050> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f18b264d0e0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f18b264d170> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f18b264d3b0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f18b264d320> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f18b264d440> ====================\n","INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n","INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f18b263d850> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n","INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n","INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f18b25c5f10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/Impulse_32)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_33))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_35))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/InitializeWrite_36))+(ref_PCollection_PCollection_21/Write))+(ref_PCollection_PCollection_22/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse_5)+(ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/Impulse_16)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_17))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_19))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/InitializeWrite_20))+(ref_PCollection_PCollection_10/Write))+(ref_PCollection_PCollection_11/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((((((((ref_PCollection_PCollection_1_split/Read)+(ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_ExtractColumns_7))+(ref_AppliedPTransform_Preprocess_8))+(ref_AppliedPTransform_Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_11))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_37))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_21))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/WriteBundles_22))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/Pair_23))+(TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/WriteBundles_38))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/Pair_39))+(EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/Extract_41))+(ref_PCollection_PCollection_27/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_21/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/PreFinalize_42))+(ref_PCollection_PCollection_28/Write)\n","WARNING:apache_beam.io.filebasedsink:Deleting 2 existing files in target path matching: -*-of-%(num_shards)05d\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_21/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/FinalizeWrite_43)\n","INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n","INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.10 seconds.\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/Extract_25))+(ref_PCollection_PCollection_16/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_10/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/PreFinalize_26))+(ref_PCollection_PCollection_17/Write)\n","WARNING:apache_beam.io.filebasedsink:Deleting 2 existing files in target path matching: -*-of-%(num_shards)05d\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_10/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/FinalizeWrite_27)\n","INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n","INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.10 seconds.\n"]}]},{"cell_type":"markdown","metadata":{"id":"wkkG271a4qnA"},"source":["### Validación preprocess test en local (0.25 puntos)\n","\n","Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."]},{"cell_type":"code","metadata":{"id":"u4lkLgecLS3i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657360332876,"user_tz":-120,"elapsed":3401,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"9418ab23-6692-4bc4-a977-69c89953a42b"},"source":["! python3 preprocess.py \\\n","  --work-dir $WORK_DIR \\\n","  --runner DirectRunner \\\n","  --input $WORK_DIR/data.json \\\n","  --output $WORK_DIR/transformed_data \\\n","  --mode test"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f60cae8cb90> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f60cae8ccb0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f60cae8cd40> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f60cae8cdd0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f60cae8ce60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f60cae8cf80> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f60cae88050> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f60cae880e0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f60cae88170> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f60cae883b0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f60cae88320> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f60cae88440> ====================\n","INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n","INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f60ca91bc10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n","INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n","INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f60ca980e10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/Impulse_13)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_14))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_16))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/InitializeWrite_17))+(ref_PCollection_PCollection_7/Write))+(ref_PCollection_PCollection_8/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse_5)+(ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((((ref_PCollection_PCollection_1_split/Read)+(ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_ExtractColumns_7))+(ref_AppliedPTransform_Preprocess_8))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_18))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/WriteBundles_19))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/Pair_20))+(TestWriteToCSV/Write/WriteImpl/GroupByKey/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((TestWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/Extract_22))+(ref_PCollection_PCollection_13/Write)\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_7/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/PreFinalize_23))+(ref_PCollection_PCollection_14/Write)\n","WARNING:apache_beam.io.filebasedsink:Deleting 2 existing files in target path matching: -*-of-%(num_shards)05d\n","INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_7/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/FinalizeWrite_24)\n","INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n","INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.10 seconds.\n"]}]},{"cell_type":"markdown","metadata":{"id":"geZM9Sbj45LK"},"source":["## Entregable 2 (1.25 puntos)\n","\n","Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n","\n","- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."]},{"cell_type":"markdown","metadata":{"id":"OMUwXgm_5el-"},"source":["Se crea el directorio donde se dejará este entregable."]},{"cell_type":"code","metadata":{"id":"HMi8dI1gLoIc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657360386032,"user_tz":-120,"elapsed":234,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"66f6964d-fb05-460f-897d-2b7efa01ceae"},"source":["%mkdir /content/batch/trainer"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/batch/trainer’: File exists\n"]}]},{"cell_type":"code","metadata":{"id":"3dJyMXTuNPwo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657360387003,"user_tz":-120,"elapsed":4,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"ea4e4ea4-caf4-47fc-b3c3-aba714e3d054"},"source":["%%writefile trainer/__init__.py\n","\n","version = \"0.1.0\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting trainer/__init__.py\n"]}]},{"cell_type":"code","metadata":{"id":"mUu6gKaTL_S2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657372128776,"user_tz":-120,"elapsed":295,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"7ce3eb05-d33d-46dc-a942-de02ead6576f"},"source":["%%writefile trainer/task.py\n","\n","from __future__ import absolute_import\n","\n","import argparse\n","import multiprocessing as mp\n","import logging\n","import tempfile\n","import os\n","import json\n","\n","import pickle\n","import gensim\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import (\n","    Dense,\n","    Dropout,\n","    Embedding,\n","    LSTM,\n",")\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","# WORD2VEC\n","W2V_SIZE = 300\n","W2V_WINDOW = 7\n","# 32\n","W2V_EPOCH = 5\n","W2V_MIN_COUNT = 10\n","\n","# KERAS\n","SEQUENCE_LENGTH = 300\n","\n","# LABEL\n","IS_TROLL = \"TROLL\"\n","NOT_TROLL = \"NOTROLL\"\n","TROLL_THRESHOLDS = 0.5\n","\n","# EXPORT\n","KERAS_MODEL = \"model.h5\"\n","WORD2VEC_MODEL = \"model.w2v\"\n","TOKENIZER_MODEL = \"tokenizer.pkl\"\n","ENCODER_MODEL = \"encoder.pkl\"\n","\n","\n","def generate_word2vec(train_df):\n","    documents = [_text.split() for _text in train_df.text.values]\n","    w2v_model = gensim.models.word2vec.Word2Vec(\n","        size=W2V_SIZE,\n","        window=W2V_WINDOW,\n","        min_count=W2V_MIN_COUNT,\n","        workers=mp.cpu_count(),\n","    )\n","    w2v_model.build_vocab(documents)\n","\n","    words = w2v_model.wv.vocab.keys()\n","    vocab_size = len(words)\n","    logging.info(f\"Vocab size: {vocab_size}\")\n","    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n","\n","    return w2v_model\n","\n","\n","def generate_tokenizer(train_df):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(train_df.text)\n","    vocab_size = len(tokenizer.word_index) + 1\n","    logging.info(f\"Total words: {vocab_size}\")\n","    return tokenizer, vocab_size\n","\n","\n","def generate_label_encoder(train_df):\n","    encoder = LabelEncoder()\n","    encoder.fit(train_df.comment_type.tolist())\n","    return encoder\n","\n","\n","def generate_embedding(word2vec_model, vocab_size, tokenizer):\n","    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n","    for word, i in tokenizer.word_index.items():\n","        if word in word2vec_model.wv:\n","            embedding_matrix[i] = word2vec_model.wv[word]\n","    return Embedding(\n","        vocab_size,\n","        W2V_SIZE,\n","        weights=[embedding_matrix],\n","        input_length=SEQUENCE_LENGTH,\n","        trainable=False,\n","    )\n","\n","\n","def train_and_evaluate(\n","    work_dir, train_df, eval_df, batch_size=1024, epochs=8, steps=1000\n","):\n","\n","    \"\"\"\n","    Trains and evaluates the estimator given.\n","    The input functions are generated by the preprocessing function.\n","    \"\"\"\n","\n","    model_dir = os.path.join(work_dir, \"model\")\n","    if tf.io.gfile.exists(model_dir):\n","        tf.io.gfile.rmtree(model_dir)\n","    tf.io.gfile.mkdir(model_dir)\n","\n","    # Specify where to store our model\n","    run_config = tf.estimator.RunConfig()\n","    run_config = run_config.replace(model_dir=model_dir)\n","\n","    # This will give us a more granular visualization of the training\n","    run_config = run_config.replace(save_summary_steps=10)\n","\n","    # Create Word2vec of training data\n","    logging.info(\"---- Generating word2vec model ----\")\n","    word2vec_model = generate_word2vec(train_df)\n","\n","    # Tokenize training data\n","    logging.info(\"---- Generating tokenizer ----\")\n","    tokenizer, vocab_size = generate_tokenizer(train_df)\n","\n","    logging.info(\"---- Tokenizing train data ----\")\n","    x_train = pad_sequences(\n","        tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH\n","    )\n","    logging.info(\"---- Tokenizing eval data ----\")\n","    x_eval = pad_sequences(\n","        tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH\n","    )\n","\n","    # Label Encoder\n","    logging.info(\"---- Generating label encoder ----\")\n","    label_encoder = generate_label_encoder(train_df)\n","\n","    logging.info(\"---- Encoding train target ----\")\n","    y_train = label_encoder.transform(train_df.comment_type.tolist())\n","    logging.info(\"---- Encoding eval target ----\")\n","    y_eval = label_encoder.transform(eval_df.comment_type.tolist())\n","\n","    y_train = y_train.reshape(-1, 1)\n","    y_eval = y_eval.reshape(-1, 1)\n","\n","    # Create Embedding Layer\n","    logging.info(\"---- Generating embedding layer ----\")\n","    embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer)\n","\n","    logging.info(\"---- Generating Sequential model ----\")\n","    model = Sequential()\n","    model.add(embedding_layer)\n","    model.add(Dropout(0.5))\n","    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","    model.add(Dense(1, activation=\"sigmoid\"))\n","\n","    model.summary()\n","\n","    logging.info(\"---- Adding loss function to model ----\")\n","    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","    logging.info(\"---- Adding callbacks to model ----\")\n","    callbacks = [\n","        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, cooldown=0),\n","        EarlyStopping(monitor=\"val_accuracy\", min_delta=1e-4, patience=5),\n","    ]\n","\n","    logging.info(\"---- Training model ----\")\n","    model.fit(\n","        x_train,\n","        y_train,\n","        batch_size=batch_size,\n","        steps_per_epoch=steps,\n","        epochs=epochs,\n","        validation_split=0.1,\n","        verbose=1,\n","        callbacks=callbacks,\n","    )\n","\n","    logging.info(\"---- Evaluating model ----\")\n","    score = model.evaluate(x_eval, y_eval, batch_size=batch_size)\n","    logging.info(f\"ACCURACY: {score[1]}\")\n","    logging.info(f\"LOSS: {score[0]}\")\n","\n","    logging.info(\"---- Saving models ----\")\n","    pickle.dump(\n","        tokenizer,\n","        tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode=\"wb\"),\n","        protocol=0,\n","    )\n","    with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n","        with tf.io.gfile.GFile(\n","            os.path.join(model_dir, KERAS_MODEL), mode=\"wb\"\n","        ) as gcs_file:\n","            model.save(local_file.name)\n","            gcs_file.write(local_file.read())\n","\n","    # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))\n","\n","    # pickle.dump(\n","    #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), \"wb\"), protocol=0\n","    # )\n","\n","\n","if __name__ == \"__main__\":\n","\n","    \"\"\"Main function called by AI Platform.\"\"\"\n","\n","    logging.getLogger().setLevel(logging.INFO)\n","\n","    parser = argparse.ArgumentParser(\n","        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n","    )\n","\n","    parser.add_argument(\n","        \"--job-dir\",\n","        help=\"Directory for staging trainer files. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--work-dir\",\n","        required=True,\n","        help=\"Directory for staging and working files. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--batch-size\",\n","        type=int,\n","        default=1024,\n","        help=\"Batch size for training and evaluation.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--epochs\", type=int, default=8, help=\"Number of epochs to train the model\",\n","    )\n","\n","    parser.add_argument(\n","        \"--steps\",\n","        type=int,\n","        default=1000,\n","        help=\"Number of steps per epoch to train the model\",\n","    )\n","\n","    args = parser.parse_args()\n","\n","    train_data_files = tf.io.gfile.glob(\n","        os.path.join(args.work_dir, \"transformed_data/train/part-*\")\n","    )\n","    eval_data_files = tf.io.gfile.glob(\n","        os.path.join(args.work_dir, \"transformed_data/eval/part-*\")\n","    )\n","\n","    train_df = pd.concat(\n","        [\n","            pd.read_csv(\n","                f,\n","                names=[\"text\", \"comment_type\"],\n","                dtype={\"text\": \"str\", \"comment_type\": \"str\"},\n","            )\n","            for f in train_data_files\n","        ]\n","    ).dropna()\n","\n","    eval_df = pd.concat(\n","        [\n","            pd.read_csv(\n","                f,\n","                names=[\"text\", \"comment_type\"],\n","                dtype={\"text\": \"str\", \"comment_type\": \"str\"},\n","            )\n","            for f in eval_data_files\n","        ]\n","    ).dropna()\n","\n","    train_and_evaluate(\n","        args.work_dir,\n","        train_df=train_df,\n","        eval_df=eval_df,\n","        batch_size=args.batch_size,\n","        epochs=args.epochs,\n","        steps=args.steps,\n","    )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting trainer/task.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"8axPHdHX6d9W"},"source":["### Validación Train en local\n","\n","Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."]},{"cell_type":"code","metadata":{"id":"m9997ZzsLmq-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657362515213,"user_tz":-120,"elapsed":62054,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"e41f2088-b145-4a64-9d23-0c3f81529429"},"source":["# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n","! gcloud config set ml_engine/local_python $(which python3)\n","\n","# This is similar to `python -m trainer.task --job-dir local-training-output`\n","# but it better replicates the AI Platform environment, especially for\n","# distributed training (not applicable here).\n","! gcloud ai-platform local train \\\n","  --package-path trainer \\\n","  --module-name trainer.task \\\n","  -- \\\n","  --work-dir $WORK_DIR \\\n","  --epochs 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [ml_engine/local_python].\n","INFO:tensorflow:TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--work-dir', '/content/batch', '--epochs', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}\n","INFO:root:---- Generating word2vec model ----\n","INFO:gensim.models.word2vec:collecting all words and their counts\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 67496 words, keeping 11647 word types\n","INFO:gensim.models.word2vec:collected 14768 word types from a corpus of 114150 raw words and 16002 sentences\n","INFO:gensim.models.word2vec:Loading a fresh vocabulary\n","INFO:gensim.models.word2vec:effective_min_count=10 retains 1611 unique words (10% of original 14768, drops 13157)\n","INFO:gensim.models.word2vec:effective_min_count=10 leaves 84645 word corpus (74% of original 114150, drops 29505)\n","INFO:gensim.models.word2vec:deleting the raw counts dictionary of 14768 items\n","INFO:gensim.models.word2vec:sample=0.001 downsamples 57 most-common words\n","INFO:gensim.models.word2vec:downsampling leaves estimated 70444 word corpus (83.2% of prior 84645)\n","INFO:gensim.models.base_any2vec:estimated required memory for 1611 words and 300 dimensions: 4671900 bytes\n","INFO:gensim.models.word2vec:resetting layer weights\n","INFO:root:Vocab size: 1611\n","INFO:gensim.models.base_any2vec:training model with 2 workers on 1611 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 114150 raw words (70429 effective words) took 0.1s, 501653 effective words/s\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 114150 raw words (70401 effective words) took 0.1s, 537229 effective words/s\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 114150 raw words (70505 effective words) took 0.1s, 517642 effective words/s\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 114150 raw words (70464 effective words) took 0.2s, 454547 effective words/s\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 114150 raw words (70397 effective words) took 0.2s, 367419 effective words/s\n","INFO:gensim.models.base_any2vec:training on a 570750 raw words (352196 effective words) took 0.8s, 444182 effective words/s\n","INFO:root:---- Generating tokenizer ----\n","INFO:root:Total words: 14769\n","INFO:root:---- Tokenizing train data ----\n","INFO:root:---- Tokenizing eval data ----\n","INFO:root:---- Generating label encoder ----\n","INFO:root:---- Encoding train target ----\n","INFO:root:---- Encoding eval target ----\n","INFO:root:---- Generating embedding layer ----\n","INFO:root:---- Generating Sequential model ----\n","2022-07-09 10:27:48.681408: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 300, 300)          4430700   \n","                                                                 \n"," dropout (Dropout)           (None, 300, 300)          0         \n","                                                                 \n"," lstm (LSTM)                 (None, 100)               160400    \n","                                                                 \n"," dense (Dense)               (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 4,591,201\n","Trainable params: 160,501\n","Non-trainable params: 4,430,700\n","_________________________________________________________________\n","INFO:root:---- Adding loss function to model ----\n","INFO:root:---- Adding callbacks to model ----\n","INFO:root:---- Training model ----\n","  15/1000 [..............................] - ETA: 20:14 - loss: 0.6898 - accuracy: 0.5523WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n","WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n","1000/1000 [==============================] - 25s 18ms/step - loss: 0.6898 - accuracy: 0.5523 - val_loss: 0.6164 - val_accuracy: 0.8582 - lr: 0.0010\n","INFO:root:---- Evaluating model ----\n","4/4 [==============================] - 1s 109ms/step - loss: 0.6692 - accuracy: 0.6157\n","INFO:root:ACCURACY: 0.6156620383262634\n","INFO:root:LOSS: 0.6691772937774658\n","INFO:root:---- Saving models ----\n"]}]},{"cell_type":"markdown","metadata":{"id":"0nhlkcjn62Zo"},"source":["## Entregable 3 (0.5 puntos)\n","\n","Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los de test generados en el primer entregable.\n"]},{"cell_type":"code","metadata":{"id":"eHBpLXB-OmjT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657363422378,"user_tz":-120,"elapsed":311,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"ec3efc9a-c797-42f6-c928-eb37050349e2"},"source":["%%writefile predict.py\n","\n","from __future__ import absolute_import\n","from __future__ import print_function\n","\n","import argparse\n","import tempfile\n","import json\n","import os\n","import sys\n","import time\n","\n","import apache_beam as beam\n","from apache_beam.io import ReadFromText\n","from apache_beam.io import WriteToText\n","from apache_beam.options.pipeline_options import GoogleCloudOptions\n","from apache_beam.options.pipeline_options import PipelineOptions\n","from apache_beam.options.pipeline_options import SetupOptions\n","from apache_beam.coders.coders import Coder\n","\n","import pickle\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# KERAS\n","SEQUENCE_LENGTH = 300\n","\n","# TYPE OF COMMENT\n","IS_TROLL = \"TROLL\"\n","NOT_TROLL = \"NOTROLL\"\n","TROLL_THRESHOLDS = 0.5\n","\n","# EXPORT\n","KERAS_MODEL = \"model.h5\"\n","TOKENIZER_MODEL = \"tokenizer.pkl\"\n","\n","\n","class Predict(beam.DoFn):\n","    def __init__(\n","        self, model_dir,\n","    ):\n","        self.model_dir = model_dir\n","        self.model = None\n","        self.tokenizer = None\n","\n","    def setup(self):\n","        keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)\n","        with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n","            with tf.io.gfile.GFile(keras_model_path, mode=\"rb\") as gcs_file:\n","                local_file.write(gcs_file.read())\n","                self.model = tf.keras.models.load_model(local_file.name)\n","\n","        tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)\n","        self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode=\"rb\"))\n","\n","    def decode_type_of_comment(self, score):\n","        label = [NOT_TROLL]\n","        if score >= TROLL_THRESHOLDS:\n","            label = IS_TROLL\n","        return label\n","\n","    def process(self, element):\n","        start_at = time.time()\n","        # Tokenize text\n","        x_test = pad_sequences(\n","            self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH\n","        )\n","        # Predict\n","        score = self.model.predict([x_test])[0]\n","        # Decode type_of_comment\n","        label = self.decode_type_of_comment(score)\n","\n","        yield {\n","            \"text\": element,\n","            \"label\": label,\n","            \"score\": float(score),\n","            \"elapsed_time\": time.time() - start_at,\n","        }\n","\n","\n","class CustomCoder(Coder):\n","    \"\"\"A custom coder used for reading and writing strings\"\"\"\n","\n","    def __init__(self, encoding: str):\n","        # latin-1\n","        # iso-8859-1\n","        self.enconding = encoding\n","\n","    def encode(self, value):\n","        return value.encode(self.enconding)\n","\n","    def decode(self, value):\n","        return value.decode(self.enconding)\n","\n","    def is_deterministic(self):\n","        return True\n","\n","\n","def run(model_dir, source, sink, beam_options=None):\n","    with beam.Pipeline(options=beam_options) as p:\n","        _ = (\n","            p\n","            | \"Read data\" >> source\n","            # | \"Preprocess\" >> beam.ParDo(PreprocessTextFn(model_dir, \"ID\"))\n","            | \"Predict\" >> beam.ParDo(Predict(model_dir))\n","            | \"Format as JSON\" >> beam.Map(json.dumps)\n","            | \"Write predictions\" >> sink\n","        )\n","\n","\n","if __name__ == \"__main__\":\n","    \"\"\"Main function\"\"\"\n","    parser = argparse.ArgumentParser(\n","        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n","    )\n","\n","    parser.add_argument(\n","        \"--work-dir\",\n","        dest=\"work_dir\",\n","        required=True,\n","        help=\"Directory for temporary files and preprocessed datasets to. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model-dir\",\n","        dest=\"model_dir\",\n","        required=True,\n","        help=\"Path to the exported TensorFlow model. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    verbs = parser.add_subparsers(dest=\"verb\")\n","    batch_verb = verbs.add_parser(\"batch\", help=\"Batch prediction\")\n","    batch_verb.add_argument(\n","        \"--inputs-dir\",\n","        dest=\"inputs_dir\",\n","        required=True,\n","        help=\"Input directory where CSV data files are read from. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","    batch_verb.add_argument(\n","        \"--outputs-dir\",\n","        dest=\"outputs_dir\",\n","        required=True,\n","        help=\"Directory to store prediction results. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    args, pipeline_args = parser.parse_known_args()\n","    print(args)\n","    beam_options = PipelineOptions(pipeline_args)\n","    beam_options.view_as(SetupOptions).save_main_session = True\n","    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n","\n","    project = beam_options.view_as(GoogleCloudOptions).project\n","\n","    if args.verb == \"batch\":\n","        results_prefix = os.path.join(args.outputs_dir, \"part\")\n","\n","        source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n","        sink = WriteToText(results_prefix)\n","\n","    else:\n","        parser.print_usage()\n","        sys.exit(1)\n","\n","    run(args.model_dir, source, sink, beam_options)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing predict.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"MUdgcLIP7a42"},"source":["Generamos un timestamp para la ejecución de las predicciones"]},{"cell_type":"code","metadata":{"id":"_pvKMtuQPOlr"},"source":["from datetime import datetime\n","\n","# current date and time\n","TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkLMdMup70ne"},"source":["### Validación Predict en local\n","\n","Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."]},{"cell_type":"code","metadata":{"id":"iUJN0XCLPR_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657364415200,"user_tz":-120,"elapsed":964211,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"d56009c7-e752-4cff-e9ea-7af936e8bd08"},"source":["! python3 predict.py \\\n","  --work-dir $WORK_DIR \\\n","  --model-dir $WORK_DIR/model \\\n","  batch \\\n","  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n","  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(inputs_dir='/content/batch/transformed_data/test/part-00000-of-00002', model_dir='/content/batch/model', outputs_dir='/content/batch/predictions/2022-07-09_10-44-03', verb='batch', work_dir='/content/batch')\n","2022-07-09 10:44:15.500907: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}]},{"cell_type":"markdown","metadata":{"id":"Um68jx_F8mjF"},"source":["##Entregable 4 (1.25 puntos)\n","\n","En este entregable se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"]},{"cell_type":"markdown","metadata":{"id":"_JHC9dCT83FH"},"source":["Establecemos el bucket y region de GCP sobre el que trabajaremos:"]},{"cell_type":"code","metadata":{"id":"cgUpX__KQvt-"},"source":["GCP_WORK_DIR = 'gs://twitch-practice-355807-irena-vent'\n","GCP_REGION = 'europe-west1'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_A66WAfm9DmU"},"source":["### Validación preprocess train en Dataflow (0.25 puntos)\n","\n","Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow."]},{"cell_type":"code","metadata":{"id":"EJYVuWyJP1Ya","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657365813357,"user_tz":-120,"elapsed":537791,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"77a94cee-5c73-43dd-aa01-77809dc9c006"},"source":["! python3 preprocess.py \\\n","  --project $PROJECT_ID \\\n","  --region $GCP_REGION \\\n","  --runner DataflowRunner \\\n","  --temp_location $GCP_WORK_DIR/beam-temp \\\n","  --setup_file ./setup.py \\\n","  --work-dir $GCP_WORK_DIR \\\n","  --input $GCP_WORK_DIR/data.json \\\n","  --output $GCP_WORK_DIR/transformed_data \\\n","  --mode train"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpsm6vady8']\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","warning: check: missing required meta-data: url\n","\n","warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n","\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsm6vady8', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsm6vady8', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n","INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://twitch-practice-355807-irena-vent/beam-temp\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/pipeline.pb in 1 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/workflow.tar.gz...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/workflow.tar.gz in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/pickled_main_session...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/pickled_main_session in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/dataflow_python_sdk.tar in 1 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709111450-696379.1657365290.696692/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 12 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," createTime: '2022-07-09T11:15:12.093190Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2022-07-09_04_15_10-18271353972805957458'\n"," location: 'europe-west1'\n"," name: 'beamapp-root-0709111450-696379'\n"," projectId: 'twitch-practice-355807'\n"," stageStates: []\n"," startTime: '2022-07-09T11:15:12.093190Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-07-09_04_15_10-18271353972805957458]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-07-09_04_15_10-18271353972805957458\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/europe-west1/2022-07-09_04_15_10-18271353972805957458?project=twitch-practice-355807\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-07-09_04_15_10-18271353972805957458 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:10.487Z: JOB_MESSAGE_BASIC: Dataflow Runner V2 auto-enabled. Use --experiments=disable_runner_v2 to opt out.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:12.644Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-07-09_04_15_10-18271353972805957458. The number of workers will be between 1 and 1000.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:12.732Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-07-09_04_15_10-18271353972805957458.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:14.595Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in europe-west1-b.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.444Z: JOB_MESSAGE_DETAILED: Expanding SplittableParDo operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.474Z: JOB_MESSAGE_DETAILED: Expanding CollectionToSingleton operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.565Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.597Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step TrainWriteToCSV/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.618Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step EvalWriteToCSV/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.657Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.722Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.757Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.784Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/InitializeWrite into EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.818Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/InitializeWrite into TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.840Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>) into EvalWriteToCSV/Write/WriteImpl/DoOnce/Impulse\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.874Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode) into EvalWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.896Z: JOB_MESSAGE_DETAILED: Fusing consumer ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction into ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.929Z: JOB_MESSAGE_DETAILED: Fusing consumer ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/SplitWithSizing into ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.973Z: JOB_MESSAGE_DETAILED: Fusing consumer ExtractColumns into ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/ProcessElementAndRestrictionWithSizing\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:15.999Z: JOB_MESSAGE_DETAILED: Fusing consumer Preprocess into ExtractColumns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.033Z: JOB_MESSAGE_DETAILED: Fusing consumer Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn) into Preprocess\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.068Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn) into Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.099Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/WriteBundles into EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.131Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/Pair into EvalWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.166Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write into EvalWriteToCSV/Write/WriteImpl/Pair\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.199Z: JOB_MESSAGE_DETAILED: Fusing consumer EvalWriteToCSV/Write/WriteImpl/Extract into EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.233Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>) into TrainWriteToCSV/Write/WriteImpl/DoOnce/Impulse\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.266Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode) into TrainWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.300Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn) into Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.322Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/WriteBundles into TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.356Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/Pair into TrainWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.381Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write into TrainWriteToCSV/Write/WriteImpl/Pair\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.404Z: JOB_MESSAGE_DETAILED: Fusing consumer TrainWriteToCSV/Write/WriteImpl/Extract into TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.444Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.469Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.503Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.537Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.669Z: JOB_MESSAGE_DEBUG: Executing wait step start28\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.757Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/DoOnce/Impulse+TrainWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)+TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)+TrainWriteToCSV/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.789Z: JOB_MESSAGE_BASIC: Executing operation ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/SplitWithSizing\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.804Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.812Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/DoOnce/Impulse+EvalWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)+EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)+EvalWriteToCSV/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:15:16.833Z: JOB_MESSAGE_BASIC: Starting 1 workers in europe-west1-b...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-07-09_04_15_10-18271353972805957458 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:16:10.258Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:16:37.294Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.240Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/DoOnce/Impulse+TrainWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)+TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)+TrainWriteToCSV/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.303Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode).None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.336Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/InitializeWrite.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.393Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.411Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.438Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.446Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.473Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.483Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.513Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/WriteBundles.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.538Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:36.568Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-TrainWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:40.153Z: JOB_MESSAGE_BASIC: Finished operation ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/SplitWithSizing\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:40.218Z: JOB_MESSAGE_DEBUG: Value \"ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6-split-with-sizing-out3\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.522Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/DoOnce/Impulse+EvalWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)+EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)+EvalWriteToCSV/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.603Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode).None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.639Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/InitializeWrite.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.706Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.730Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.752Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.798Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.810Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.836Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/WriteBundles.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.845Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.886Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.918Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.954Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:43.988Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-EvalWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:46.617Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:46.685Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/GroupByKey/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:49.583Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:49.667Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/GroupByKey/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:21:49.726Z: JOB_MESSAGE_BASIC: Executing operation ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/ProcessElementAndRestrictionWithSizing+ExtractColumns+Preprocess+Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+EvalWriteToCSV/Write/WriteImpl/WriteBundles+EvalWriteToCSV/Write/WriteImpl/Pair+EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write+TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TrainWriteToCSV/Write/WriteImpl/WriteBundles+TrainWriteToCSV/Write/WriteImpl/Pair+TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:05.184Z: JOB_MESSAGE_BASIC: Finished operation ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/ProcessElementAndRestrictionWithSizing+ExtractColumns+Preprocess+Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+EvalWriteToCSV/Write/WriteImpl/WriteBundles+EvalWriteToCSV/Write/WriteImpl/Pair+EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write+TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TrainWriteToCSV/Write/WriteImpl/WriteBundles+TrainWriteToCSV/Write/WriteImpl/Pair+TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:05.232Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:05.265Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:05.289Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:05.352Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read+EvalWriteToCSV/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:05.526Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:05.593Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read+TrainWriteToCSV/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.403Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read+EvalWriteToCSV/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.444Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/Extract.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.487Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.519Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-EvalWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.527Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.566Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-EvalWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.593Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.614Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-EvalWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:10.668Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:13.816Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read+TrainWriteToCSV/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:13.872Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/Extract.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:13.927Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:13.949Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-TrainWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:13.978Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:14.022Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-TrainWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:14.035Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:14.077Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-TrainWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:14.133Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:17.519Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:17.580Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/PreFinalize.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:17.647Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:17.683Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:17.736Z: JOB_MESSAGE_DEBUG: Value \"TrainWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-TrainWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:17.836Z: JOB_MESSAGE_BASIC: Executing operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:21.209Z: JOB_MESSAGE_BASIC: Finished operation TrainWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:25.771Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:25.836Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/PreFinalize.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:25.903Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:25.952Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:26.002Z: JOB_MESSAGE_DEBUG: Value \"EvalWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-EvalWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:26.072Z: JOB_MESSAGE_BASIC: Executing operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:29.517Z: JOB_MESSAGE_BASIC: Finished operation EvalWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:29.606Z: JOB_MESSAGE_DEBUG: Executing success step success26\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:29.678Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:30.289Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:22:30.318Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:23:22.350Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:23:22.394Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:23:22.430Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-07-09_04_15_10-18271353972805957458 is in state JOB_STATE_DONE\n"]}]},{"cell_type":"markdown","metadata":{"id":"8czKRMb99ZRX"},"source":["### Validación preprocess test en Dataflow (0.25 puntos)\n","\n","Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow."]},{"cell_type":"code","metadata":{"id":"45mfSRGdQLBO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657366400945,"user_tz":-120,"elapsed":539666,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"aee6768e-0369-49e2-ddcc-6938530b723f"},"source":["! python3 preprocess.py \\\n","  --project $PROJECT_ID \\\n","  --region $GCP_REGION \\\n","  --runner DataflowRunner \\\n","  --temp_location $GCP_WORK_DIR/beam-temp \\\n","  --setup_file ./setup.py \\\n","  --work-dir $GCP_WORK_DIR \\\n","  --input $GCP_WORK_DIR/data.json \\\n","  --output $GCP_WORK_DIR/transformed_data \\\n","  --mode test"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpsob14kus']\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","warning: check: missing required meta-data: url\n","\n","warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n","\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsob14kus', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsob14kus', 'apache-beam==2.24.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n","INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.24.0. If the image is not available at local, we will try to pull from hub.docker.com\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://twitch-practice-355807-irena-vent/beam-temp\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/pipeline.pb in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/workflow.tar.gz...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/workflow.tar.gz in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/pickled_main_session...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/pickled_main_session in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/dataflow_python_sdk.tar in 1 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://twitch-practice-355807-irena-vent/beam-temp/beamapp-root-0709112434-833222.1657365874.833512/apache_beam-2.24.0-cp37-cp37m-manylinux1_x86_64.whl in 12 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," createTime: '2022-07-09T11:24:54.556016Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2022-07-09_04_24_53-8086087505571910396'\n"," location: 'europe-west1'\n"," name: 'beamapp-root-0709112434-833222'\n"," projectId: 'twitch-practice-355807'\n"," stageStates: []\n"," startTime: '2022-07-09T11:24:54.556016Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-07-09_04_24_53-8086087505571910396]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-07-09_04_24_53-8086087505571910396\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/europe-west1/2022-07-09_04_24_53-8086087505571910396?project=twitch-practice-355807\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-07-09_04_24_53-8086087505571910396 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:53.930Z: JOB_MESSAGE_BASIC: Dataflow Runner V2 auto-enabled. Use --experiments=disable_runner_v2 to opt out.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:55.053Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-07-09_04_24_53-8086087505571910396. The number of workers will be between 1 and 1000.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:55.109Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-07-09_04_24_53-8086087505571910396.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:57.051Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in europe-west1-d.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:57.892Z: JOB_MESSAGE_DETAILED: Expanding SplittableParDo operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:57.962Z: JOB_MESSAGE_DETAILED: Expanding CollectionToSingleton operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.102Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.139Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step TestWriteToCSV/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.171Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.208Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.265Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.284Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/InitializeWrite into TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.308Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>) into TestWriteToCSV/Write/WriteImpl/DoOnce/Impulse\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.337Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode) into TestWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.368Z: JOB_MESSAGE_DETAILED: Fusing consumer ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction into ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.402Z: JOB_MESSAGE_DETAILED: Fusing consumer ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/SplitWithSizing into ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.434Z: JOB_MESSAGE_DETAILED: Fusing consumer ExtractColumns into ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/ProcessElementAndRestrictionWithSizing\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.468Z: JOB_MESSAGE_DETAILED: Fusing consumer Preprocess into ExtractColumns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.502Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn) into Preprocess\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.535Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/WriteBundles into TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.568Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/Pair into TestWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.602Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/GroupByKey/Write into TestWriteToCSV/Write/WriteImpl/Pair\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.637Z: JOB_MESSAGE_DETAILED: Fusing consumer TestWriteToCSV/Write/WriteImpl/Extract into TestWriteToCSV/Write/WriteImpl/GroupByKey/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.680Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.713Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.745Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:58.827Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:59.002Z: JOB_MESSAGE_DEBUG: Executing wait step start19\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:59.078Z: JOB_MESSAGE_BASIC: Executing operation ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/SplitWithSizing\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:59.112Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/DoOnce/Impulse+TestWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)+TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)+TestWriteToCSV/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:59.127Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:24:59.152Z: JOB_MESSAGE_BASIC: Starting 1 workers in europe-west1-d...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-07-09_04_24_53-8086087505571910396 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:25:29.785Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:26:08.028Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.603Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/DoOnce/Impulse+TestWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)+TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)+TestWriteToCSV/Write/WriteImpl/InitializeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.660Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode).None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.739Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/InitializeWrite.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.800Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.877Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.908Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.911Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/WriteBundles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.939Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.961Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.969Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/WriteBundles/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/WriteBundles.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:45.995Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:46.021Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input0-TestWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:49.668Z: JOB_MESSAGE_BASIC: Finished operation ReadTwitterData/Read/_SDFBoundedSourceWrapper/Impulse+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/PairWithRestriction+ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/SplitWithSizing\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:49.757Z: JOB_MESSAGE_DEBUG: Value \"ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6-split-with-sizing-out3\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:49.879Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:52.599Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:52.663Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/GroupByKey/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:31:52.752Z: JOB_MESSAGE_BASIC: Executing operation ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/ProcessElementAndRestrictionWithSizing+ExtractColumns+Preprocess+TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TestWriteToCSV/Write/WriteImpl/WriteBundles+TestWriteToCSV/Write/WriteImpl/Pair+TestWriteToCSV/Write/WriteImpl/GroupByKey/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:06.859Z: JOB_MESSAGE_BASIC: Finished operation ref_AppliedPTransform_ReadTwitterData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)_6/ProcessElementAndRestrictionWithSizing+ExtractColumns+Preprocess+TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)+TestWriteToCSV/Write/WriteImpl/WriteBundles+TestWriteToCSV/Write/WriteImpl/Pair+TestWriteToCSV/Write/WriteImpl/GroupByKey/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:06.930Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:07.478Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:07.543Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Read+TestWriteToCSV/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.661Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/GroupByKey/Read+TestWriteToCSV/Write/WriteImpl/Extract\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.759Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/Extract.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.804Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.877Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-TestWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.903Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.928Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-TestWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.952Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input1-TestWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:10.984Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/PreFinalize/View-python_side_input1-TestWriteToCSV/Write/WriteImpl/PreFinalize.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:11.038Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:14.453Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/PreFinalize\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:14.515Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/PreFinalize.None\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:14.578Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:14.639Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:14.748Z: JOB_MESSAGE_DEBUG: Value \"TestWriteToCSV/Write/WriteImpl/FinalizeWrite/View-python_side_input2-TestWriteToCSV/Write/WriteImpl/FinalizeWrite.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:14.794Z: JOB_MESSAGE_BASIC: Executing operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:18.111Z: JOB_MESSAGE_BASIC: Finished operation TestWriteToCSV/Write/WriteImpl/FinalizeWrite\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:18.173Z: JOB_MESSAGE_DEBUG: Executing success step success17\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:18.244Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:18.381Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:32:18.414Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:33:10.291Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:33:10.386Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-07-09T11:33:10.420Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-07-09_04_24_53-8086087505571910396 is in state JOB_STATE_DONE\n"]}]},{"cell_type":"markdown","metadata":{"id":"NFNxYtxW9fP9"},"source":["### Validación Train en AI Platform (0.5 puntos)\n","\n","Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos de las ejecuciones anteriores en GCP con los datos obtenidos almacenados en Google Cloud Storage."]},{"cell_type":"markdown","metadata":{"id":"b0fP_5Gy9zfz"},"source":["Generamos un nombre para el job de entrenamiento y donde se almacenarán los metadatos."]},{"cell_type":"code","metadata":{"id":"_eU0FkyLQPt6"},"source":["JOB = \"troll_detection_batch_$(date +%Y%m%d_%H%M%S)\"\n","JOB_DIR = GCP_WORK_DIR + \"/trainer\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Z33nVs5QSEI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657372662219,"user_tz":-120,"elapsed":497459,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"2675428b-782d-4d4f-9244-f12a1f6b97db"},"source":["! gcloud ai-platform jobs submit training $JOB \\\n","  --module-name trainer.task \\\n","  --package-path trainer \\\n","  --scale-tier basic_gpu \\\n","  --python-version 3.7 \\\n","  --runtime-version 2.1 \\\n","  --region $GCP_REGION \\\n","  --job-dir $JOB_DIR \\\n","  --stream-logs \\\n","  -- \\\n","  --work-dir $GCP_WORK_DIR \\\n","  --epochs 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Job [troll_detection_batch_20220709_130924] submitted successfully.\n","INFO\t2022-07-09 13:09:29 +0000\tservice\t\tValidating job requirements...\n","INFO\t2022-07-09 13:09:30 +0000\tservice\t\tJob creation request has been successfully validated.\n","INFO\t2022-07-09 13:09:30 +0000\tservice\t\tJob troll_detection_batch_20220709_130924 is queued.\n","INFO\t2022-07-09 13:09:30 +0000\tservice\t\tWaiting for job to be provisioned.\n","INFO\t2022-07-09 13:09:32 +0000\tservice\t\tWaiting for training program to start.\n","NOTICE\t2022-07-09 13:10:23 +0000\tmaster-replica-0.gcsfuse\t\tOpening GCS connection...\n","NOTICE\t2022-07-09 13:10:23 +0000\tmaster-replica-0.gcsfuse\t\tMounting file system \"gcsfuse\"...\n","NOTICE\t2022-07-09 13:10:23 +0000\tmaster-replica-0.gcsfuse\t\tFile system has been successfully mounted.\n","INFO\t2022-07-09 13:10:29 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"chief\": [\"127.0.0.1:2222\"]} --task={\"type\": \"chief\", \"index\": 0} --job={  \"scale_tier\": \"BASIC_GPU\",  \"package_uris\": [\"gs://twitch-practice-355807-irena-vent/trainer/packages/c36052ece3ce0bee7604047585dc5f9d380cc483489a0bf0afaa10267a850c41/twitchstreaming-0.0.1.tar.gz\"],  \"python_module\": \"trainer.task\",  \"args\": [\"--work-dir\", \"gs://twitch-practice-355807-irena-vent\", \"--epochs\", \"1\"],  \"region\": \"europe-west1\",  \"runtime_version\": \"2.1\",  \"job_dir\": \"gs://twitch-practice-355807-irena-vent/trainer\",  \"run_on_raw_vm\": true,  \"python_version\": \"3.7\"}\n","WARNING\t2022-07-09 13:10:36 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","WARNING\t2022-07-09 13:10:36 +0000\tmaster-replica-0\t\tInstructions for updating:\n","WARNING\t2022-07-09 13:10:36 +0000\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\n","INFO\t2022-07-09 13:10:40 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n","INFO\t2022-07-09 13:10:40 +0000\tmaster-replica-0\t\tDownloading the package: gs://twitch-practice-355807-irena-vent/trainer/packages/c36052ece3ce0bee7604047585dc5f9d380cc483489a0bf0afaa10267a850c41/twitchstreaming-0.0.1.tar.gz\n","INFO\t2022-07-09 13:10:40 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://twitch-practice-355807-irena-vent/trainer/packages/c36052ece3ce0bee7604047585dc5f9d380cc483489a0bf0afaa10267a850c41/twitchstreaming-0.0.1.tar.gz twitchstreaming-0.0.1.tar.gz\n","INFO\t2022-07-09 13:10:41 +0000\tmaster-replica-0\t\tInstalling the package: gs://twitch-practice-355807-irena-vent/trainer/packages/c36052ece3ce0bee7604047585dc5f9d380cc483489a0bf0afaa10267a850c41/twitchstreaming-0.0.1.tar.gz\n","INFO\t2022-07-09 13:10:41 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user --upgrade --force-reinstall --no-deps twitchstreaming-0.0.1.tar.gz\n","INFO\t2022-07-09 13:10:42 +0000\tmaster-replica-0\t\tProcessing ./twitchstreaming-0.0.1.tar.gz\n","INFO\t2022-07-09 13:10:44 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: twitchstreaming\n","INFO\t2022-07-09 13:10:44 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): started\n","INFO\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\t  Created wheel for twitchstreaming: filename=twitchstreaming-0.0.1-py3-none-any.whl size=3989 sha256=067c86bf1d0b86d1343345566ac9e46fb1dbeb2faf6087a10def5baf5f77961c\n","INFO\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/c3/74/78/094c3d65891811370d10f2463da6ec5f32ea469d2e14797ac9\n","INFO\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\tSuccessfully built twitchstreaming\n","INFO\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\tInstalling collected packages: twitchstreaming\n","INFO\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\tSuccessfully installed twitchstreaming-0.0.1\n","ERROR\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 22.1.2 is available.\n","ERROR\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n","INFO\t2022-07-09 13:10:46 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user twitchstreaming-0.0.1.tar.gz\n","INFO\t2022-07-09 13:10:47 +0000\tmaster-replica-0\t\tProcessing ./twitchstreaming-0.0.1.tar.gz\n","INFO\t2022-07-09 13:10:49 +0000\tmaster-replica-0\t\tCollecting apache-beam[gcp]==2.24.0\n","INFO\t2022-07-09 13:10:49 +0000\tmaster-replica-0\t\t  Downloading apache_beam-2.24.0-cp37-cp37m-manylinux2010_x86_64.whl (8.5 MB)\n","INFO\t2022-07-09 13:10:50 +0000\tmaster-replica-0\t\tCollecting tensorflow==2.8.0\n","INFO\t2022-07-09 13:10:50 +0000\tmaster-replica-0\t\t  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n","INFO\t2022-07-09 13:11:13 +0000\tmaster-replica-0\t\tCollecting gensim==3.6.0\n","INFO\t2022-07-09 13:11:13 +0000\tmaster-replica-0\t\t  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n","INFO\t2022-07-09 13:11:16 +0000\tmaster-replica-0\t\tCollecting fsspec==0.8.4\n","INFO\t2022-07-09 13:11:16 +0000\tmaster-replica-0\t\t  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n","INFO\t2022-07-09 13:11:17 +0000\tmaster-replica-0\t\tCollecting gcsfs==0.7.1\n","INFO\t2022-07-09 13:11:17 +0000\tmaster-replica-0\t\t  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n","INFO\t2022-07-09 13:11:17 +0000\tmaster-replica-0\t\tCollecting numpy==1.20.0\n","INFO\t2022-07-09 13:11:17 +0000\tmaster-replica-0\t\t  Downloading numpy-1.20.0-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n","INFO\t2022-07-09 13:11:18 +0000\tmaster-replica-0\t\tCollecting keras==2.8.0\n","INFO\t2022-07-09 13:11:18 +0000\tmaster-replica-0\t\t  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","INFO\t2022-07-09 13:11:18 +0000\tmaster-replica-0\t\tRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2.8.1)\n","INFO\t2022-07-09 13:11:18 +0000\tmaster-replica-0\t\tCollecting pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n","INFO\t2022-07-09 13:11:18 +0000\tmaster-replica-0\t\t  Downloading pyarrow-0.17.1-cp37-cp37m-manylinux2014_x86_64.whl (63.8 MB)\n","INFO\t2022-07-09 13:11:21 +0000\tmaster-replica-0\t\tRequirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.15.0)\n","INFO\t2022-07-09 13:11:22 +0000\tmaster-replica-0\t\tCollecting typing-extensions<3.8.0,>=3.7.0\n","INFO\t2022-07-09 13:11:22 +0000\tmaster-replica-0\t\t  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","INFO\t2022-07-09 13:11:22 +0000\tmaster-replica-0\t\tCollecting mock<3.0.0,>=1.0.1\n","INFO\t2022-07-09 13:11:22 +0000\tmaster-replica-0\t\t  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n","INFO\t2022-07-09 13:11:22 +0000\tmaster-replica-0\t\tCollecting pymongo<4.0.0,>=3.8.0\n","INFO\t2022-07-09 13:11:22 +0000\tmaster-replica-0\t\t  Downloading pymongo-3.12.3-cp37-cp37m-manylinux2014_x86_64.whl (527 kB)\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\tCollecting grpcio<2,>=1.29.0\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\t  Downloading grpcio-1.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\tCollecting pydot<2,>=1.2.0\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\t  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\tCollecting requests<3.0.0,>=2.24.0\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\t  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\tRequirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (3.20.1)\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\tCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\"\n","INFO\t2022-07-09 13:11:24 +0000\tmaster-replica-0\t\t  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n","INFO\t2022-07-09 13:11:28 +0000\tmaster-replica-0\t\tCollecting dill<0.3.2,>=0.3.1.1\n","INFO\t2022-07-09 13:11:28 +0000\tmaster-replica-0\t\t  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","INFO\t2022-07-09 13:11:31 +0000\tmaster-replica-0\t\tCollecting oauth2client<4,>=2.0.1\n","INFO\t2022-07-09 13:11:31 +0000\tmaster-replica-0\t\t  Downloading oauth2client-3.0.0.tar.gz (77 kB)\n","INFO\t2022-07-09 13:11:33 +0000\tmaster-replica-0\t\tCollecting fastavro<0.24,>=0.21.4\n","INFO\t2022-07-09 13:11:33 +0000\tmaster-replica-0\t\t  Downloading fastavro-0.23.6-cp37-cp37m-manylinux2010_x86_64.whl (1.4 MB)\n","INFO\t2022-07-09 13:11:33 +0000\tmaster-replica-0\t\tRequirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.18.2)\n","INFO\t2022-07-09 13:11:33 +0000\tmaster-replica-0\t\tCollecting hdfs<3.0.0,>=2.1.0\n","INFO\t2022-07-09 13:11:33 +0000\tmaster-replica-0\t\t  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n","INFO\t2022-07-09 13:11:34 +0000\tmaster-replica-0\t\tRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.7)\n","INFO\t2022-07-09 13:11:34 +0000\tmaster-replica-0\t\tRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2022.1)\n","INFO\t2022-07-09 13:11:34 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-auth<2,>=1.18.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.35.0)\n","INFO\t2022-07-09 13:11:34 +0000\tmaster-replica-0\t\tCollecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:34 +0000\tmaster-replica-0\t\t  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n","INFO\t2022-07-09 13:11:35 +0000\tmaster-replica-0\t\tCollecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:35 +0000\tmaster-replica-0\t\t  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n","INFO\t2022-07-09 13:11:37 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.23.1)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading google_cloud_spanner-1.19.3-py2.py3-none-any.whl (255 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting cachetools<4,>=3.1.0; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.10.0)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.7.2)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.1.0)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.2.0)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading google_cloud_dlp-1.0.2-py2.py3-none-any.whl (169 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (2.10.0)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (1.1.0)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting keras-preprocessing>=1.1.1\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting astunparse>=1.6.0\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\tCollecting tensorboard<2.9,>=2.8\n","INFO\t2022-07-09 13:11:38 +0000\tmaster-replica-0\t\t  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\tRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (1.11.2)\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\tRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (3.3.0)\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\tCollecting tf-estimator-nightly==2.8.0.dev2021122109\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\t  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\tRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (1.13.0)\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\tRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (1.1.0)\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\tCollecting libclang>=9.0.1\n","INFO\t2022-07-09 13:11:39 +0000\tmaster-replica-0\t\t  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tCollecting tensorflow-io-gcs-filesystem>=0.23.1\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\t  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (0.2.2)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (0.2.0)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0->twitchstreaming==0.0.1) (62.6.0)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tCollecting flatbuffers>=1.12\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\t  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->twitchstreaming==0.0.1) (1.4.1)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tCollecting smart_open>=1.2.1\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\t  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->twitchstreaming==0.0.1) (0.4.6)\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\tCollecting decorator\n","INFO\t2022-07-09 13:11:40 +0000\tmaster-replica-0\t\t  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tCollecting aiohttp\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\t  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tCollecting pbr>=0.11\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\t  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (3.0.9)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tCollecting charset-normalizer<3,>=2\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\t  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2.8)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.25.11)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (2022.6.15)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.4.8)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.2.8)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (4.8)\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\tCollecting docopt\n","INFO\t2022-07-09 13:11:41 +0000\tmaster-replica-0\t\t  Downloading docopt-0.6.2.tar.gz (25 kB)\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision<2,>=0.38.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.31.6)\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\tCollecting fasteners>=0.14\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\t  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\tRequirement already satisfied: google-resumable-media!=0.4.0,<0.6.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.5.1)\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\tRequirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (0.12.4)\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\tRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0->twitchstreaming==0.0.1) (0.33.6)\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\tCollecting tensorboard-data-server<0.7.0,>=0.6.0\n","INFO\t2022-07-09 13:11:43 +0000\tmaster-replica-0\t\t  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->twitchstreaming==0.0.1) (2.1.2)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tCollecting tensorboard-plugin-wit>=1.6.0\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\t  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->twitchstreaming==0.0.1) (3.3.7)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->twitchstreaming==0.0.1) (1.3.1)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tCollecting frozenlist>=1.1.1\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\t  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tCollecting attrs>=17.3.0\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\t  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tCollecting async-timeout<5.0,>=4.0.0a3\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\t  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\tCollecting aiosignal>=1.1.2\n","INFO\t2022-07-09 13:11:44 +0000\tmaster-replica-0\t\t  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tCollecting multidict<7.0,>=4.5\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\t  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tCollecting asynctest==0.13.0; python_version < \"3.8\"\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\t  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tCollecting yarl<2.0,>=1.0\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\t  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-vision<2,>=0.38.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (1.56.3)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-vision<2,>=0.38.0; extra == \"gcp\"->apache-beam[gcp]==2.24.0->twitchstreaming==0.0.1) (21.3)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0->twitchstreaming==0.0.1) (4.12.0)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->twitchstreaming==0.0.1) (3.2.0)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0->twitchstreaming==0.0.1) (3.8.0)\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: twitchstreaming, gensim, avro-python3, dill, oauth2client, google-apitools, docopt\n","INFO\t2022-07-09 13:11:45 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): started\n","INFO\t2022-07-09 13:11:48 +0000\tmaster-replica-0\t\t  Building wheel for twitchstreaming (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:11:48 +0000\tmaster-replica-0\t\t  Created wheel for twitchstreaming: filename=twitchstreaming-0.0.1-py3-none-any.whl size=3989 sha256=8c451c1ba5095b94328956be657f1b2c89ad93a0a5014e12dc624b4dde3d38fb\n","INFO\t2022-07-09 13:11:48 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/c3/74/78/094c3d65891811370d10f2463da6ec5f32ea469d2e14797ac9\n","INFO\t2022-07-09 13:11:48 +0000\tmaster-replica-0\t\t  Building wheel for gensim (setup.py): started\n","INFO\t2022-07-09 13:12:09 +0000\tmaster-replica-0\t\t  Building wheel for gensim (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:12:09 +0000\tmaster-replica-0\t\t  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24214505 sha256=b6a5807fbe7028e545955539a62311f15fbba159cf9facbb38f3ed64c89052eb\n","INFO\t2022-07-09 13:12:09 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/53/c8/f9/afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\n","INFO\t2022-07-09 13:12:09 +0000\tmaster-replica-0\t\t  Building wheel for avro-python3 (setup.py): started\n","INFO\t2022-07-09 13:12:11 +0000\tmaster-replica-0\t\t  Building wheel for avro-python3 (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:12:11 +0000\tmaster-replica-0\t\t  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43497 sha256=bb68f1e2e26cf5e52ff16b113051e8bedf3335031640fd22a823cab0b7692d33\n","INFO\t2022-07-09 13:12:11 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n","INFO\t2022-07-09 13:12:11 +0000\tmaster-replica-0\t\t  Building wheel for dill (setup.py): started\n","INFO\t2022-07-09 13:12:14 +0000\tmaster-replica-0\t\t  Building wheel for dill (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:12:14 +0000\tmaster-replica-0\t\t  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78543 sha256=1e081cd00c8b4ecc2376448d9cd11c9a2a553b5c8701c841dbcef543323e3413\n","INFO\t2022-07-09 13:12:14 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n","INFO\t2022-07-09 13:12:14 +0000\tmaster-replica-0\t\t  Building wheel for oauth2client (setup.py): started\n","INFO\t2022-07-09 13:12:16 +0000\tmaster-replica-0\t\t  Building wheel for oauth2client (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:12:16 +0000\tmaster-replica-0\t\t  Created wheel for oauth2client: filename=oauth2client-3.0.0-py3-none-any.whl size=106359 sha256=2a6ff5d96f6785ab39b593f9852f50ed0f9fc888ce33bdac8ee6d218529f3dae\n","INFO\t2022-07-09 13:12:16 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/86/73/7a/3b3f76a2142176605ff38fbca574327962c71e25a43197a4c1\n","INFO\t2022-07-09 13:12:16 +0000\tmaster-replica-0\t\t  Building wheel for google-apitools (setup.py): started\n","INFO\t2022-07-09 13:12:19 +0000\tmaster-replica-0\t\t  Building wheel for google-apitools (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:12:19 +0000\tmaster-replica-0\t\t  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131021 sha256=006c37928b13b8462233ddd43af142db4e3d6b61e5792977cca1583257a4c871\n","INFO\t2022-07-09 13:12:19 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n","INFO\t2022-07-09 13:12:19 +0000\tmaster-replica-0\t\t  Building wheel for docopt (setup.py): started\n","INFO\t2022-07-09 13:12:21 +0000\tmaster-replica-0\t\t  Building wheel for docopt (setup.py): finished with status 'done'\n","INFO\t2022-07-09 13:12:21 +0000\tmaster-replica-0\t\t  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=3572f1bcc49a2e5b246b3eedee0b16e1098ea48897850e1d3be2e32bea3c27c8\n","INFO\t2022-07-09 13:12:21 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n","INFO\t2022-07-09 13:12:21 +0000\tmaster-replica-0\t\tSuccessfully built twitchstreaming gensim avro-python3 dill oauth2client google-apitools docopt\n","ERROR\t2022-07-09 13:12:22 +0000\tmaster-replica-0\t\tERROR: tensorflow-gpu 2.1.3 has requirement keras-preprocessing==1.1.0, but you'll have keras-preprocessing 1.1.2 which is incompatible.\n","ERROR\t2022-07-09 13:12:22 +0000\tmaster-replica-0\t\tERROR: tensorflow-gpu 2.1.3 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.20.0 which is incompatible.\n","ERROR\t2022-07-09 13:12:22 +0000\tmaster-replica-0\t\tERROR: tensorflow-gpu 2.1.3 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.8.0 which is incompatible.\n","INFO\t2022-07-09 13:12:22 +0000\tmaster-replica-0\t\tInstalling collected packages: numpy, pyarrow, typing-extensions, pbr, mock, pymongo, grpcio, pydot, charset-normalizer, requests, avro-python3, dill, oauth2client, fastavro, docopt, hdfs, google-cloud-vision, fasteners, google-apitools, google-cloud-spanner, cachetools, grpcio-gcp, google-cloud-language, google-cloud-videointelligence, google-cloud-dlp, apache-beam, keras, keras-preprocessing, astunparse, tensorboard-data-server, tensorboard-plugin-wit, tensorboard, tf-estimator-nightly, libclang, tensorflow-io-gcs-filesystem, flatbuffers, tensorflow, smart-open, gensim, fsspec, decorator, frozenlist, attrs, async-timeout, aiosignal, multidict, asynctest, yarl, aiohttp, gcsfs, twitchstreaming\n","ERROR\t2022-07-09 13:12:24 +0000\tmaster-replica-0\t\t  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:24 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:27 +0000\tmaster-replica-0\t\t  WARNING: The script plasma_store is installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:27 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:27 +0000\tmaster-replica-0\t\t  WARNING: The script pbr is installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:27 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:27 +0000\tmaster-replica-0\t\t  WARNING: The script normalizer is installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:27 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:28 +0000\tmaster-replica-0\t\t  WARNING: The script fastavro is installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:28 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:28 +0000\tmaster-replica-0\t\t  WARNING: The scripts hdfscli and hdfscli-avro are installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:28 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:28 +0000\tmaster-replica-0\t\t  WARNING: The script gen_client is installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:28 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:33 +0000\tmaster-replica-0\t\t  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:33 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","ERROR\t2022-07-09 13:12:53 +0000\tmaster-replica-0\t\t  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n","ERROR\t2022-07-09 13:12:53 +0000\tmaster-replica-0\t\t  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","INFO\t2022-07-09 13:12:55 +0000\tmaster-replica-0\t\t  Attempting uninstall: twitchstreaming\n","INFO\t2022-07-09 13:12:55 +0000\tmaster-replica-0\t\t    Found existing installation: twitchstreaming 0.0.1\n","INFO\t2022-07-09 13:12:55 +0000\tmaster-replica-0\t\t    Uninstalling twitchstreaming-0.0.1:\n","INFO\t2022-07-09 13:12:55 +0000\tmaster-replica-0\t\t      Successfully uninstalled twitchstreaming-0.0.1\n","INFO\t2022-07-09 13:12:55 +0000\tmaster-replica-0\t\tSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 apache-beam-2.24.0 astunparse-1.6.3 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.4.0 avro-python3-1.9.2.1 cachetools-3.1.1 charset-normalizer-2.1.0 decorator-5.1.1 dill-0.3.1.1 docopt-0.6.2 fastavro-0.23.6 fasteners-0.17.3 flatbuffers-2.0 frozenlist-1.3.0 fsspec-0.8.4 gcsfs-0.7.1 gensim-3.6.0 google-apitools-0.5.31 google-cloud-dlp-1.0.2 google-cloud-language-1.3.2 google-cloud-spanner-1.19.3 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 grpcio-1.47.0 grpcio-gcp-0.2.2 hdfs-2.7.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 mock-2.0.0 multidict-6.0.2 numpy-1.20.0 oauth2client-3.0.0 pbr-5.9.0 pyarrow-0.17.1 pydot-1.4.2 pymongo-3.12.3 requests-2.28.1 smart-open-6.0.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.26.0 tf-estimator-nightly-2.8.0.dev2021122109 twitchstreaming-0.0.1 typing-extensions-3.7.4.3 yarl-1.7.2\n","ERROR\t2022-07-09 13:12:55 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 22.1.2 is available.\n","ERROR\t2022-07-09 13:12:55 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n","INFO\t2022-07-09 13:12:56 +0000\tmaster-replica-0\t\tRunning command: python3 -m trainer.task --work-dir gs://twitch-practice-355807-irena-vent --epochs 1 --job-dir gs://twitch-practice-355807-irena-vent/trainer\n","INFO\t2022-07-09 13:12:57 +0000\tmaster-replica-0\t\t'pattern' package not found; tag filters are not available for English\n","WARNING\t2022-07-09 13:12:58 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64\n","INFO\t2022-07-09 13:12:58 +0000\tmaster-replica-0\t\tIgnore above cudart dlerror if you do not have a GPU set up on your machine.\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tTF_CONFIG environment variable: {'cluster': {'chief': ['127.0.0.1:2222']}, 'task': {'type': 'chief', 'index': 0, 'cloud': 'w6505837fc231ce9ep-tp'}, 'job': {'scale_tier': 'BASIC_GPU', 'package_uris': ['gs://twitch-practice-355807-irena-vent/trainer/packages/c36052ece3ce0bee7604047585dc5f9d380cc483489a0bf0afaa10267a850c41/twitchstreaming-0.0.1.tar.gz'], 'python_module': 'trainer.task', 'args': ['--work-dir', 'gs://twitch-practice-355807-irena-vent', '--epochs', '1', '--job-dir', 'gs://twitch-practice-355807-irena-vent/trainer'], 'region': 'europe-west1', 'runtime_version': '2.1', 'job_dir': 'gs://twitch-practice-355807-irena-vent/trainer', 'run_on_raw_vm': True, 'python_version': '3.7'}, 'environment': 'cloud'}\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\t---- Generating word2vec model ----\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tcollecting all words and their counts\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tPROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tPROGRESS: at sentence #10000, processed 79032 words, keeping 10810 word types\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tcollected 14756 word types from a corpus of 114076 raw words and 15926 sentences\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tLoading a fresh vocabulary\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\teffective_min_count=10 retains 1593 unique words (10% of original 14756, drops 13163)\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\teffective_min_count=10 leaves 84564 word corpus (74% of original 114076, drops 29512)\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tdeleting the raw counts dictionary of 14756 items\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tsample=0.001 downsamples 56 most-common words\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tdownsampling leaves estimated 70414 word corpus (83.3% of prior 84564)\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\testimated required memory for 1593 words and 300 dimensions: 4619700 bytes\n","INFO\t2022-07-09 13:13:00 +0000\tmaster-replica-0\t\tresetting layer weights\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tVocab size: 1593\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\ttraining model with 8 workers on 1593 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tEPOCH - 1 : training on 114076 raw words (70360 effective words) took 0.1s, 907931 effective words/s\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tEPOCH - 2 : training on 114076 raw words (70263 effective words) took 0.1s, 1042531 effective words/s\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tEPOCH - 3 : training on 114076 raw words (70479 effective words) took 0.1s, 987271 effective words/s\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tEPOCH - 4 : training on 114076 raw words (70368 effective words) took 0.1s, 1055736 effective words/s\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 7 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 6 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 5 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 4 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 3 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 2 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 1 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tworker thread finished; awaiting finish of 0 more threads\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tEPOCH - 5 : training on 114076 raw words (70481 effective words) took 0.1s, 986210 effective words/s\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\ttraining on a 570380 raw words (351951 effective words) took 0.4s, 872237 effective words/s\n","WARNING\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tunder 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\t---- Generating tokenizer ----\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\tTotal words: 14757\n","INFO\t2022-07-09 13:13:01 +0000\tmaster-replica-0\t\t---- Tokenizing train data ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Tokenizing eval data ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Generating label encoder ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Encoding train target ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Encoding eval target ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Generating embedding layer ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Generating Sequential model ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","ERROR\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tSkipping registering GPU devices...\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tThis TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","ERROR\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tModel: \"sequential\"\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t_________________________________________________________________\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t Layer (type)                Output Shape              Param #   \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t=================================================================\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t embedding (Embedding)       (None, 300, 300)          4427100   \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Adding loss function to model ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t                                                                 \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t dropout (Dropout)           (None, 300, 300)          0         \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t                                                                 \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t lstm (LSTM)                 (None, 100)               160400    \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t                                                                 \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t dense (Dense)               (None, 1)                 101       \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t                                                                 \n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t=================================================================\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tTotal params: 4,587,601\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tTrainable params: 160,501\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tNon-trainable params: 4,427,100\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t_________________________________________________________________\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Adding callbacks to model ----\n","INFO\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\t---- Training model ----\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tAutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f87e5394200> and will run it as-is.\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tCause: 'arguments' object has no attribute 'posonlyargs'\n","WARNING\t2022-07-09 13:13:02 +0000\tmaster-replica-0\t\tTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","INFO\t2022-07-09 13:13:14 +0000\tmaster-replica-0\t\t   1/1000 [..............................] - ETA: 2:04:03 - loss: 0.6937 - accuracy: 0.4980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:18 +0000\tmaster-replica-0\t\t   2/1000 [..............................] - ETA: 1:08:54 - loss: 0.6941 - accuracy: 0.5317\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:22 +0000\tmaster-replica-0\t\t   3/1000 [..............................] - ETA: 1:08:21 - loss: 0.6914 - accuracy: 0.5492\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:26 +0000\tmaster-replica-0\t\t   4/1000 [..............................] - ETA: 1:07:36 - loss: 0.6909 - accuracy: 0.5537\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:30 +0000\tmaster-replica-0\t\t   5/1000 [..............................] - ETA: 1:07:20 - loss: 0.6899 - accuracy: 0.5604\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:34 +0000\tmaster-replica-0\t\t   6/1000 [..............................] - ETA: 1:06:52 - loss: 0.6904 - accuracy: 0.5575\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:37 +0000\tmaster-replica-0\t\t   7/1000 [..............................] - ETA: 1:06:33 - loss: 0.6904 - accuracy: 0.5540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:42 +0000\tmaster-replica-0\t\t   8/1000 [..............................] - ETA: 1:06:18 - loss: 0.6905 - accuracy: 0.5535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:45 +0000\tmaster-replica-0\t\t   9/1000 [..............................] - ETA: 1:06:20 - loss: 0.6902 - accuracy: 0.5560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:49 +0000\tmaster-replica-0\t\t  10/1000 [..............................] - ETA: 1:06:04 - loss: 0.6901 - accuracy: 0.5562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:53 +0000\tmaster-replica-0\t\t  11/1000 [..............................] - ETA: 1:05:51 - loss: 0.6897 - accuracy: 0.5578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:13:57 +0000\tmaster-replica-0\t\t  12/1000 [..............................] - ETA: 1:05:39 - loss: 0.6895 - accuracy: 0.5591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:14:01 +0000\tmaster-replica-0\t\t  13/1000 [..............................] - ETA: 1:05:29 - loss: 0.6901 - accuracy: 0.5570\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","WARNING\t2022-07-09 13:14:01 +0000\tmaster-replica-0\t\tYour input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n","WARNING\t2022-07-09 13:14:01 +0000\tmaster-replica-0\t\tAutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f87d4330dd0> and will run it as-is.\n","WARNING\t2022-07-09 13:14:01 +0000\tmaster-replica-0\t\tPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","WARNING\t2022-07-09 13:14:01 +0000\tmaster-replica-0\t\tCause: 'arguments' object has no attribute 'posonlyargs'\n","WARNING\t2022-07-09 13:14:01 +0000\tmaster-replica-0\t\tTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","INFO\t2022-07-09 13:14:03 +0000\tmaster-replica-0\t\t  14/1000 [..............................] - ETA: 1:05:13 - loss: 0.6898 - accuracy: 0.5577\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:14:03 +0000\tmaster-replica-0\t\t1000/1000 [==============================] - 61s 54ms/step - loss: 0.6898 - accuracy: 0.5577 - val_loss: 0.6132 - val_accuracy: 1.0000 - lr: 0.0010\n","INFO\t2022-07-09 13:14:03 +0000\tmaster-replica-0\t\t---- Evaluating model ----\n","INFO\t2022-07-09 13:14:05 +0000\tmaster-replica-0\t\t1/4 [======>.......................] - ETA: 2s - loss: 0.7768 - accuracy: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:14:06 +0000\tmaster-replica-0\t\t2/4 [==============>...............] - ETA: 1s - loss: 0.7397 - accuracy: 0.2314    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:14:07 +0000\tmaster-replica-0\t\t3/4 [=====================>........] - ETA: 0s - loss: 0.6982 - accuracy: 0.4876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:14:07 +0000\tmaster-replica-0\t\t4/4 [==============================] - ETA: 0s - loss: 0.6788 - accuracy: 0.6023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n","INFO\t2022-07-09 13:14:07 +0000\tmaster-replica-0\t\t4/4 [==============================] - 4s 881ms/step - loss: 0.6788 - accuracy: 0.6023\n","INFO\t2022-07-09 13:14:07 +0000\tmaster-replica-0\t\tACCURACY: 0.6023244261741638\n","INFO\t2022-07-09 13:14:07 +0000\tmaster-replica-0\t\tLOSS: 0.6787632703781128\n","INFO\t2022-07-09 13:14:07 +0000\tmaster-replica-0\t\t---- Saving models ----\n","INFO\t2022-07-09 13:14:08 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n","INFO\t2022-07-09 13:14:08 +0000\tmaster-replica-0\t\tClean up finished.\n","INFO\t2022-07-09 13:14:08 +0000\tmaster-replica-0\t\tTask completed successfully.\n","endTime: '2022-07-09T13:17:37'\n","jobId: troll_detection_batch_20220709_130924\n","startTime: '2022-07-09T13:10:33'\n","state: SUCCEEDED\n"]}]},{"cell_type":"markdown","metadata":{"id":"wu5SOWsy9886"},"source":["### Validación predict en Dataflow (0.25 puntos)\n","\n","Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."]},{"cell_type":"markdown","metadata":{"id":"OXoHCgMg-LUD"},"source":["Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."]},{"cell_type":"code","metadata":{"id":"fmcetQtnQjVo"},"source":["from datetime import datetime\n","\n","# current date and time\n","TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zT_NxsKDQlxD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"79e1a9c9-8dff-439f-fe38-8de6dcd53fb2"},"source":["# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n","! python3 predict.py \\\n","  --work-dir $GCP_WORK_DIR \\\n","  --model-dir $GCP_WORK_DIR/model/ \\\n","  batch \\\n","  --project $PROJECT_ID \\\n","  --region $GCP_REGION \\\n","  --runner DataflowRunner \\\n","  --temp_location $GCP_WORK_DIR/beam-temp \\\n","  --setup_file ./setup.py \\\n","  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n","  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(inputs_dir='gs://twitch-practice-355807-irena-vent/transformed_data/test/part*', model_dir='gs://twitch-practice-355807-irena-vent/model/', outputs_dir='gs://twitch-practice-355807-irena-vent/predictions/2022-07-09_13-37-44', verb='batch', work_dir='gs://twitch-practice-355807-irena-vent')\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","warning: check: missing required meta-data: url\n","\n","warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n","\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"]}]},{"cell_type":"markdown","metadata":{"id":"9GWbfFvwQTGS"},"source":["# Inferencia online\n","\n","En esta segunda parte de la práctica se realizará un microservicio de inferencia online usando los modelos generados en la primera parte. Para esta parte de la práctica el código de vuestro microservicio deberá estar subido en un repositorio. En la variable de debajo deberéis dejar la URL a vuestro repositorrio pues será el contenido con el que serás evaluado. \n","\n","**Importante:** asegúrate de crear el repositorio de manera pública para poder clonarlo.\n","\n","A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n","\n","![online_diagram](https://drive.google.com/uc?export=view&id=1zR7Cwp0Vq1QeTxwLoJ8YJNRM9G5KVh2S)"]},{"cell_type":"code","metadata":{"id":"REn23OCIBjXF","executionInfo":{"status":"ok","timestamp":1657438523074,"user_tz":-120,"elapsed":263,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"source":["REPOSITORIO = \"https://github.com/IrenaVent/12-despliegue-de-algoritmos/\""],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqBT-b_8BsIR"},"source":["Creamos el directorio donde trabajaremos."]},{"cell_type":"code","metadata":{"id":"VAskyBUAQZHx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657438822265,"user_tz":-120,"elapsed":233,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"a56ada79-8b4e-4b97-bc36-8514166664bc"},"source":["%mkdir /content/online\n","%cd /content/online"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/online’: File exists\n","/content/online\n"]}]},{"cell_type":"code","metadata":{"id":"xlj7lGW3Q6CS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657438836070,"user_tz":-120,"elapsed":577,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"e54eab93-e4bf-4603-b807-da75e82435b9"},"source":["# Clone the repository\n","! git clone $REPOSITORIO\n","\n","# Set the working directory to the sample code directory\n","%cd ./12-despliegue-de-algoritmos\n","\n","# Change to develop\n","! git checkout develop"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: You must specify a repository to clone.\n","\n","usage: git clone [<options>] [--] <repo> [<dir>]\n","\n","    -v, --verbose         be more verbose\n","    -q, --quiet           be more quiet\n","    --progress            force progress reporting\n","    -n, --no-checkout     don't create a checkout\n","    --bare                create a bare repository\n","    --mirror              create a mirror repository (implies bare)\n","    -l, --local           to clone from a local repository\n","    --no-hardlinks        don't use local hardlinks, always copy\n","    -s, --shared          setup as shared repository\n","    --recurse-submodules[=<pathspec>]\n","                          initialize submodules in the clone\n","    -j, --jobs <n>        number of submodules cloned in parallel\n","    --template <template-directory>\n","                          directory from which templates will be used\n","    --reference <repo>    reference repository\n","    --reference-if-able <repo>\n","                          reference repository\n","    --dissociate          use --reference only while cloning\n","    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n","    -b, --branch <branch>\n","                          checkout <branch> instead of the remote's HEAD\n","    -u, --upload-pack <path>\n","                          path to git-upload-pack on the remote\n","    --depth <depth>       create a shallow clone of that depth\n","    --shallow-since <time>\n","                          create a shallow clone since a specific time\n","    --shallow-exclude <revision>\n","                          deepen history of shallow clone, excluding rev\n","    --single-branch       clone only one branch, HEAD or --branch\n","    --no-tags             don't clone any tags, and make later fetches not to follow them\n","    --shallow-submodules  any cloned submodules will be shallow\n","    --separate-git-dir <gitdir>\n","                          separate git dir from working tree\n","    -c, --config <key=value>\n","                          set config inside the new repository\n","    -4, --ipv4            use IPv4 addresses only\n","    -6, --ipv6            use IPv6 addresses only\n","    --filter <args>       object filtering\n","\n","/content/online/12-despliegue-de-algoritmos\n","error: pathspec 'develop' did not match any file(s) known to git.\n"]}]},{"cell_type":"code","metadata":{"id":"FndkRszYQ8_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657438846695,"user_tz":-120,"elapsed":6862,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"b0885a53-e3a7-45b4-9c2b-ba0108a4dab0"},"source":["! pip install -r requirements.txt"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: requests==2.25.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.25.0)\n","Requirement already satisfied: uvicorn==0.12.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.12.2)\n","Requirement already satisfied: fastapi==0.61.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.61.2)\n","Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.10.0)\n","Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.23.2)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.4.1)\n","Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.1.0)\n","Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.5.3)\n","Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.6.0)\n","Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.8.4)\n","Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.7.1)\n","Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.19.5)\n","Requirement already satisfied: protobuf==3.20.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (3.20.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (2022.6.15)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.12.2->-r requirements.txt (line 2)) (0.13.0)\n","Requirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.12.2->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.12.2->-r requirements.txt (line 2)) (4.1.1)\n","Requirement already satisfied: starlette==0.13.6 in /usr/local/lib/python3.7/dist-packages (from fastapi==0.61.2->-r requirements.txt (line 3)) (0.13.6)\n","Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastapi==0.61.2->-r requirements.txt (line 3)) (1.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirements.txt (line 5)) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirements.txt (line 5)) (3.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.1.2)\n","Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (2.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.14.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.37.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (3.3.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.46.3)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.0.8)\n","Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (2.1.1)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.2.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.2.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 9)) (5.2.1)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (1.35.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (4.4.2)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (0.4.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (3.8.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (57.4.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (4.2.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.4.8)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (3.3.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (1.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (3.8.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 11)) (3.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.7.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (2.1.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (21.4.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.3.0)\n"]}]},{"cell_type":"code","metadata":{"id":"IefrYp6tJ4PE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657438866461,"user_tz":-120,"elapsed":4399,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"03ba2800-18ec-44c8-9910-c576ea5c3128"},"source":["! pip install pyngrok"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyngrok\n","  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n","\u001b[K     |████████████████████████████████| 745 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=71d73540f0ea401816b50c4458090e2c9fe8152e323a7632b7163d6c991f7716\n","  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-5.1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"qNde3TEzCmjg"},"source":["Para cuando estes modificando, probando y ejecutando ficheros os dejo en las celdas de abajo los comandos de git necesarios para interaccionar con vuestro repositorio en caso de que queráis:"]},{"cell_type":"code","metadata":{"id":"qsjWKcYZC6mm"},"source":["! git status"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuVHFtqvC8Kw"},"source":["! git add <files>\n","! git commit -m \"Nuevos cambios\"\n","! git push origin master"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H88vcTU_DDpv"},"source":["Será necesario definir y establecer la variable de entorno `DEFAULT_MODEL_PATH` para definir donde están almacenados nuestros modelos para hacer inferencia."]},{"cell_type":"code","metadata":{"id":"Tn9xsgx4u5xP","executionInfo":{"status":"ok","timestamp":1657439054264,"user_tz":-120,"elapsed":323,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"source":["import os\n","\n","os.environ[\"DEFAULT_MODEL_PATH\"] = \"/content/batch/model/\""],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-IslHWwDSTU"},"source":["### Validación inferencia online en local (1.75 puntos)\n","\n","Se validará la correcta inferencia del microservio en local utilizando Swagger. Para ejecutar en local solo hay que ejecutar los comandos a continuación. Después, entrar en la URL proporcionada por ngrock `<ngrok_url>/docs` para acceder a swagger y probar la inferencia como vimos en clase."]},{"cell_type":"code","metadata":{"id":"81vGToBWJ_h-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657439059787,"user_tz":-120,"elapsed":2710,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"dbc6dab3-63d7-4bac-a174-500bff2c8726"},"source":["# For testing purposes\n","import nest_asyncio\n","from pyngrok import ngrok, conf\n","\n","conf.get_default().auth_token = \"2Bi9ATXLBsjvgEcyBNk8eDf3kC7_3AzmkK7K599ekEbyem82r\"\n","\n","ngrok_tunnel = ngrok.connect(8000)\n","print('Public URL:', ngrok_tunnel.public_url)\n","nest_asyncio.apply()\n","\n","! ngrok authtoken 2Bi9ATXLBsjvgEcyBNk8eDf3kC7_3AzmkK7K599ekEbyem82r"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Public URL: http://5eaf-34-73-195-104.ngrok.io\n","Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"]}]},{"cell_type":"code","metadata":{"id":"hFeSh7rcuG7L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657439192307,"user_tz":-120,"elapsed":66538,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"d476584f-050e-4266-b01f-623d48e876ba"},"source":["! uvicorn app.main:app --port 8000"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-07-10 07:45:26.608614: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2022-07-10 07:45:26.608720: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2022-07-10 07:45:26.608740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m785\u001b[0m]\n","\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n","\u001b[32m2022-07-10 07:45:27.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.event_handlers\u001b[0m:\u001b[36mstartup\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mRunning app start handler.\u001b[0m\n","2022-07-10 07:45:27.540983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2022-07-10 07:45:27.604753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:27.605674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n","coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n","2022-07-10 07:45:27.621602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-07-10 07:45:27.865729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-07-10 07:45:27.890609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2022-07-10 07:45:27.908091: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2022-07-10 07:45:28.138296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2022-07-10 07:45:28.157999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2022-07-10 07:45:28.525253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-07-10 07:45:28.525469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.526240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.526773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n","2022-07-10 07:45:28.527106: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2022-07-10 07:45:28.531554: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2022-07-10 07:45:28.531869: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28436c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2022-07-10 07:45:28.531904: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2022-07-10 07:45:28.764165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.765006: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2843880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2022-07-10 07:45:28.765041: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2022-07-10 07:45:28.765252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.765837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n","coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n","2022-07-10 07:45:28.765902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-07-10 07:45:28.765944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-07-10 07:45:28.765969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2022-07-10 07:45:28.765992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2022-07-10 07:45:28.766017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2022-07-10 07:45:28.766038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2022-07-10 07:45:28.766078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-07-10 07:45:28.766166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.766743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.767256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n","2022-07-10 07:45:28.767325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-07-10 07:45:28.768538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2022-07-10 07:45:28.768570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n","2022-07-10 07:45:28.768586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n","2022-07-10 07:45:28.768745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.770482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-10 07:45:28.771400: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2022-07-10 07:45:28.771458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n","\u001b[32mINFO\u001b[0m:     Application startup complete.\n","\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n","\u001b[32mINFO\u001b[0m:     Shutting down\n","\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m785\u001b[0m]\n"]}]},{"cell_type":"markdown","metadata":{"id":"fuLtOjukEMIl"},"source":["### Validación inferencia online en GCP (1.75 puntos)\n","\n","Se validará el correcto funcionamiento del microservicio haciendo una petición POST de inferencia a través de curl al microservicio desplegado en GCP."]},{"cell_type":"markdown","metadata":{"id":"MX1MiTGuD2KS"},"source":["Primero, contruiremos una imagen Docker con el microservicio y subiremos el desarrollo al Container Repository en GCP a través de Cloud Build."]},{"cell_type":"code","source":["print(f\"Project: {PROJECT_ID}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"craFmsKnwi9T","executionInfo":{"status":"ok","timestamp":1657439210348,"user_tz":-120,"elapsed":342,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"d4a3393f-9c09-474f-8426-a55493738331"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Project: twitch-practice-355807\n"]}]},{"cell_type":"code","metadata":{"id":"PRhq-d_E7-CY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657384018836,"user_tz":-120,"elapsed":354748,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"86759eb0-2691-46b5-d309-dcbdb1ec28cd"},"source":["! gcloud builds submit --tag gcr.io/$PROJECT_ID/troll-detection-online-service"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating temporary tarball archive of 40 file(s) totalling 18.2 KiB before compression.\n","Uploading tarball of [.] to [gs://twitch-practice-355807_cloudbuild/source/1657383665.543571-df40a2db694443fcb4136f0c6406e6ed.tgz]\n","Created [https://cloudbuild.googleapis.com/v1/projects/twitch-practice-355807/locations/global/builds/bae6618f-c587-4bbc-b003-a8055259dd9a].\n","Logs are available at [https://console.cloud.google.com/cloud-build/builds/bae6618f-c587-4bbc-b003-a8055259dd9a?project=250374766550].\n"," REMOTE BUILD OUTPUT\n","starting build \"bae6618f-c587-4bbc-b003-a8055259dd9a\"\n","\n","FETCHSOURCE\n","Fetching storage object: gs://twitch-practice-355807_cloudbuild/source/1657383665.543571-df40a2db694443fcb4136f0c6406e6ed.tgz#1657383666827052\n","Copying gs://twitch-practice-355807_cloudbuild/source/1657383665.543571-df40a2db694443fcb4136f0c6406e6ed.tgz#1657383666827052...\n","/ [1 files][  7.7 KiB/  7.7 KiB]                                                \n","Operation completed over 1 objects/7.7 KiB.\n","BUILD\n","Already have image (with digest): gcr.io/cloud-builders/docker\n","Sending build context to Docker daemon  55.81kB\n","Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n","python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n","df5590a8898b: Pulling fs layer\n","705bb4cb554e: Pulling fs layer\n","519df5fceacd: Pulling fs layer\n","ccc287cbeddc: Pulling fs layer\n","e3f8e6af58ed: Pulling fs layer\n","aebed27b2d86: Pulling fs layer\n","3b81dff756ff: Pulling fs layer\n","6e4f59a23b58: Pulling fs layer\n","5cdb61eb416d: Pulling fs layer\n","415755650c07: Pulling fs layer\n","82af7648a68a: Pulling fs layer\n","697e3cdf6fdb: Pulling fs layer\n","8d414b25a011: Pulling fs layer\n","3e3cfa737f56: Pulling fs layer\n","1595db7eb9b1: Pulling fs layer\n","d2d3fcd4b870: Pulling fs layer\n","35f87fd78dc8: Pulling fs layer\n","146e266752fc: Pulling fs layer\n","57df244389c0: Pulling fs layer\n","e8391aa94779: Pulling fs layer\n","ccc287cbeddc: Waiting\n","8d414b25a011: Waiting\n","e3f8e6af58ed: Waiting\n","aebed27b2d86: Waiting\n","3b81dff756ff: Waiting\n","6e4f59a23b58: Waiting\n","5cdb61eb416d: Waiting\n","415755650c07: Waiting\n","82af7648a68a: Waiting\n","697e3cdf6fdb: Waiting\n","3e3cfa737f56: Waiting\n","1595db7eb9b1: Waiting\n","d2d3fcd4b870: Waiting\n","35f87fd78dc8: Waiting\n","146e266752fc: Waiting\n","57df244389c0: Waiting\n","e8391aa94779: Waiting\n","705bb4cb554e: Verifying Checksum\n","705bb4cb554e: Download complete\n","519df5fceacd: Verifying Checksum\n","519df5fceacd: Download complete\n","df5590a8898b: Verifying Checksum\n","df5590a8898b: Download complete\n","ccc287cbeddc: Verifying Checksum\n","ccc287cbeddc: Download complete\n","aebed27b2d86: Verifying Checksum\n","aebed27b2d86: Download complete\n","6e4f59a23b58: Verifying Checksum\n","6e4f59a23b58: Download complete\n","3b81dff756ff: Verifying Checksum\n","3b81dff756ff: Download complete\n","415755650c07: Verifying Checksum\n","415755650c07: Download complete\n","5cdb61eb416d: Verifying Checksum\n","5cdb61eb416d: Download complete\n","697e3cdf6fdb: Verifying Checksum\n","697e3cdf6fdb: Download complete\n","82af7648a68a: Verifying Checksum\n","82af7648a68a: Download complete\n","8d414b25a011: Verifying Checksum\n","8d414b25a011: Download complete\n","3e3cfa737f56: Verifying Checksum\n","3e3cfa737f56: Download complete\n","1595db7eb9b1: Verifying Checksum\n","1595db7eb9b1: Download complete\n","d2d3fcd4b870: Verifying Checksum\n","d2d3fcd4b870: Download complete\n","35f87fd78dc8: Verifying Checksum\n","35f87fd78dc8: Download complete\n","146e266752fc: Verifying Checksum\n","146e266752fc: Download complete\n","e8391aa94779: Verifying Checksum\n","e8391aa94779: Download complete\n","57df244389c0: Verifying Checksum\n","57df244389c0: Download complete\n","e3f8e6af58ed: Verifying Checksum\n","e3f8e6af58ed: Download complete\n","df5590a8898b: Pull complete\n","705bb4cb554e: Pull complete\n","519df5fceacd: Pull complete\n","ccc287cbeddc: Pull complete\n","e3f8e6af58ed: Pull complete\n","aebed27b2d86: Pull complete\n","3b81dff756ff: Pull complete\n","6e4f59a23b58: Pull complete\n","5cdb61eb416d: Pull complete\n","415755650c07: Pull complete\n","82af7648a68a: Pull complete\n","697e3cdf6fdb: Pull complete\n","8d414b25a011: Pull complete\n","3e3cfa737f56: Pull complete\n","1595db7eb9b1: Pull complete\n","d2d3fcd4b870: Pull complete\n","35f87fd78dc8: Pull complete\n","146e266752fc: Pull complete\n","57df244389c0: Pull complete\n","e8391aa94779: Pull complete\n","Digest: sha256:f5770422a8875fe3d677e1bda724eeba332b53b0a348a9cdde854ba7136c66be\n","Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n"," ---> bc3531b1f244\n","Step 2/4 : COPY ./requirements.txt /requirements.txt\n"," ---> d10df14b7fd3\n","Step 3/4 : RUN pip install -r /requirements.txt\n"," ---> Running in 22a31f735e6c\n","Collecting requests==2.25.0\n","  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n","Collecting uvicorn==0.12.2\n","  Downloading uvicorn-0.12.2-py3-none-any.whl (45 kB)\n","Collecting fastapi==0.61.2\n","  Downloading fastapi-0.61.2-py3-none-any.whl (48 kB)\n","Collecting h5py==2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","Collecting scikit-learn==0.23.2\n","  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n","Collecting scipy==1.4.1\n","  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n","Collecting tensorflow==2.1.0\n","  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n","Collecting loguru==0.5.3\n","  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n","Collecting gensim==3.6.0\n","  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n","Collecting fsspec==0.8.4\n","  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n","Collecting gcsfs==0.7.1\n","  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n","Collecting numpy==1.19.5\n","  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n","Collecting idna<3,>=2.5\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","Collecting certifi>=2017.4.17\n","  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n","Collecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n","Collecting chardet<4,>=3.0.2\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (3.10.0.2)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (0.12.0)\n","Collecting click==7.*\n","  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n","Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from fastapi==0.61.2->-r /requirements.txt (line 3)) (1.8.2)\n","Collecting starlette==0.13.6\n","  Downloading starlette-0.13.6-py3-none-any.whl (59 kB)\n","Collecting six\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n","Collecting joblib>=0.11\n","  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n","Collecting tensorboard<2.2.0,>=2.1.0\n","  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n","Collecting google-pasta>=0.1.6\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","Collecting absl-py>=0.7.0\n","  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n","Collecting astor>=0.6.0\n","  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n","Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n","  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n","Collecting wrapt>=1.11.1\n","  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n","Collecting protobuf>=3.8.0\n","  Downloading protobuf-4.21.2-cp37-abi3-manylinux2014_x86_64.whl (407 kB)\n","Collecting opt-einsum>=2.3.2\n","  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","Collecting grpcio>=1.8.6\n","  Downloading grpcio-1.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","Collecting termcolor>=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.1.0->-r /requirements.txt (line 7)) (0.37.0)\n","Collecting keras-preprocessing>=1.1.0\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Collecting smart_open>=1.2.1\n","  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n","Collecting google-auth-oauthlib\n","  Downloading google_auth_oauthlib-0.5.2-py2.py3-none-any.whl (19 kB)\n","Collecting google-auth>=1.2\n","  Downloading google_auth-2.9.0-py2.py3-none-any.whl (167 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","Collecting decorator\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting rsa<5,>=3.1.4\n","  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n","Collecting pyasn1-modules>=0.2.1\n","  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n","Collecting cachetools<6.0,>=2.0.0\n","  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n","Collecting pyasn1<0.5.0,>=0.4.6\n","  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n","Collecting google-auth-oauthlib\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (57.5.0)\n","Collecting werkzeug>=0.11.15\n","  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n","Collecting google-auth>=1.2\n","  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n","Collecting markdown>=2.6.8\n","  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n","Collecting cachetools<6.0,>=2.0.0\n","  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n","Collecting requests-oauthlib>=0.7.0\n","  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (4.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (3.6.0)\n","Collecting oauthlib>=3.0.0\n","  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","Collecting attrs>=17.3.0\n","  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","Collecting charset-normalizer<3.0,>=2.0\n","  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Building wheels for collected packages: gensim, gast, termcolor\n","  Building wheel for gensim (setup.py): started\n","  Building wheel for gensim (setup.py): finished with status 'done'\n","  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24602087 sha256=786c590f80e1b47a28b2f9ec83b9ed1bcc6caaa69169f6851ebef5a4b468cc45\n","  Stored in directory: /root/.cache/pip/wheels/53/c8/f9/afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\n","  Building wheel for gast (setup.py): started\n","  Building wheel for gast (setup.py): finished with status 'done'\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=f2e25d8c44b342c5649127ab5523d6b70afc4d6965e2c7298d1bbcd24039824f\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","  Building wheel for termcolor (setup.py): started\n","  Building wheel for termcolor (setup.py): finished with status 'done'\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=2ea1fd1789185bef8322a4976ae28bc8e99d3da9bf67b8e8042b861e7dc1eee3\n","  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n","Successfully built gensim gast termcolor\n","Installing collected packages: urllib3, pyasn1, idna, chardet, certifi, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, numpy, multidict, google-auth, frozenlist, yarl, werkzeug, protobuf, markdown, h5py, grpcio, google-auth-oauthlib, charset-normalizer, attrs, asynctest, async-timeout, aiosignal, absl-py, wrapt, threadpoolctl, termcolor, tensorflow-estimator, tensorboard, starlette, smart-open, scipy, opt-einsum, keras-preprocessing, keras-applications, joblib, google-pasta, gast, fsspec, decorator, click, astor, aiohttp, uvicorn, tensorflow, scikit-learn, loguru, gensim, gcsfs, fastapi\n","  Attempting uninstall: starlette\n","    Found existing installation: starlette 0.14.2\n","    Uninstalling starlette-0.14.2:\n","      Successfully uninstalled starlette-0.14.2\n","  Attempting uninstall: click\n","    Found existing installation: click 8.0.1\n","    Uninstalling click-8.0.1:\n","      Successfully uninstalled click-8.0.1\n","  Attempting uninstall: uvicorn\n","    Found existing installation: uvicorn 0.15.0\n","    Uninstalling uvicorn-0.15.0:\n","      Successfully uninstalled uvicorn-0.15.0\n","  Attempting uninstall: fastapi\n","    Found existing installation: fastapi 0.68.1\n","    Uninstalling fastapi-0.68.1:\n","      Successfully uninstalled fastapi-0.68.1\n","Successfully installed absl-py-1.1.0 aiohttp-3.8.1 aiosignal-1.2.0 astor-0.8.1 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.4.0 cachetools-4.2.4 certifi-2022.6.15 chardet-3.0.4 charset-normalizer-2.1.0 click-7.1.2 decorator-5.1.1 fastapi-0.61.2 frozenlist-1.3.0 fsspec-0.8.4 gast-0.2.2 gcsfs-0.7.1 gensim-3.6.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.47.0 h5py-2.10.0 idna-2.10 joblib-1.1.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 loguru-0.5.3 markdown-3.3.7 multidict-6.0.2 numpy-1.19.5 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-4.21.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.0 requests-oauthlib-1.3.1 rsa-4.8 scikit-learn-0.23.2 scipy-1.4.1 six-1.16.0 smart-open-6.0.0 starlette-0.13.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 threadpoolctl-3.1.0 urllib3-1.26.10 uvicorn-0.12.2 werkzeug-2.1.2 wrapt-1.14.1 yarl-1.7.2\n","\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 22.1.2 is available.\n","You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n","\u001b[0mRemoving intermediate container 22a31f735e6c\n"," ---> 7b9b212f7d72\n","Step 4/4 : COPY ./app /app/app\n"," ---> 3359492aa968\n","Successfully built 3359492aa968\n","Successfully tagged gcr.io/twitch-practice-355807/troll-detection-online-service:latest\n","PUSH\n","Pushing gcr.io/twitch-practice-355807/troll-detection-online-service\n","The push refers to repository [gcr.io/twitch-practice-355807/troll-detection-online-service]\n","87685fa9b361: Preparing\n","f081cff5f2df: Preparing\n","ca8b876783d3: Preparing\n","574be7dd3a19: Preparing\n","4a24608ea27b: Preparing\n","c660563b938b: Preparing\n","e313cca5ff4a: Preparing\n","ad6638039b02: Preparing\n","4c503f7b7cec: Preparing\n","49022fdb47b6: Preparing\n","f4cc5099892d: Preparing\n","471ef6255d6e: Preparing\n","0f8cd2835d79: Preparing\n","0616239ad62a: Preparing\n","4eb436881a0f: Preparing\n","7fe8548565b4: Preparing\n","8364a493536b: Preparing\n","c1792902851c: Preparing\n","c272c95c3fb0: Preparing\n","3054497613e6: Preparing\n","d35dc7f4c79e: Preparing\n","dabfe5b2ea81: Preparing\n","5e6a409f30b6: Preparing\n","c660563b938b: Waiting\n","e313cca5ff4a: Waiting\n","ad6638039b02: Waiting\n","4c503f7b7cec: Waiting\n","49022fdb47b6: Waiting\n","f4cc5099892d: Waiting\n","471ef6255d6e: Waiting\n","0f8cd2835d79: Waiting\n","0616239ad62a: Waiting\n","4eb436881a0f: Waiting\n","7fe8548565b4: Waiting\n","8364a493536b: Waiting\n","c1792902851c: Waiting\n","c272c95c3fb0: Waiting\n","3054497613e6: Waiting\n","d35dc7f4c79e: Waiting\n","dabfe5b2ea81: Waiting\n","5e6a409f30b6: Waiting\n","87685fa9b361: Pushed\n","574be7dd3a19: Pushed\n","ca8b876783d3: Pushed\n","ad6638039b02: Pushed\n","e313cca5ff4a: Pushed\n","4a24608ea27b: Pushed\n","c660563b938b: Pushed\n","4c503f7b7cec: Pushed\n","f4cc5099892d: Pushed\n","471ef6255d6e: Pushed\n","49022fdb47b6: Pushed\n","7fe8548565b4: Pushed\n","0616239ad62a: Pushed\n","c1792902851c: Layer already exists\n","c272c95c3fb0: Layer already exists\n","3054497613e6: Layer already exists\n","4eb436881a0f: Pushed\n","d35dc7f4c79e: Layer already exists\n","dabfe5b2ea81: Layer already exists\n","5e6a409f30b6: Layer already exists\n","0f8cd2835d79: Pushed\n","8364a493536b: Pushed\n","f081cff5f2df: Pushed\n","latest: digest: sha256:bdfbfc71d235feed08112f9a1b2d278ac7b95a8078d4fad96762ecef0516e133 size: 5134\n","DONE\n","\n","ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                IMAGES                                                                  STATUS\n","bae6618f-c587-4bbc-b003-a8055259dd9a  2022-07-09T16:21:08+00:00  5M46S     gs://twitch-practice-355807_cloudbuild/source/1657383665.543571-df40a2db694443fcb4136f0c6406e6ed.tgz  gcr.io/twitch-practice-355807/troll-detection-online-service (+1 more)  SUCCESS\n"]}]},{"cell_type":"markdown","metadata":{"id":"aYVptQ0WEEd-"},"source":["Desplegaremos la imagen Docker generada en el Container Registry en el servicio de Cloud Run. Después, validaremos que las inferencias funcionan en GCP usando el comando mostrado a continuación:"]},{"cell_type":"code","metadata":{"id":"KC0honui-W23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657445322735,"user_tz":-120,"elapsed":2707,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"8e72b5f6-7dc1-4a63-e6d8-729cc503634e"},"source":["! curl -X POST \"https://troll-detection-online-service-73bjy6adhq-ew.a.run.app/api/model/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"text\\\":\\\"i hate you\\\"}\""],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Service Unavailable"]}]},{"cell_type":"markdown","metadata":{"id":"ZvBWPoGGKwuJ"},"source":["# Detección de Trolls en Twitch en Streaming\n","\n","En esta última parte de la práctica se realizará un pipeline de inferencia en tiempo real de un chat de Twitch alcualmente en vivo. Para ello, usaremos mi canal de Twitch `https://www.twitch.tv/franalgaba` donde tengo un bot deplegado poniendo mensajes troll y no troll de forma aleatoria del dataset que hemos usado en la primera parte.\n","\n","Para acceder al chat de Twitch os proporciono el conector correspondiente que será desplegado como Cloud Function como hicimos en clase y usando mis credenciales recogerá los mensajes del chat y los enviará a un topic de Pub/Sub en GCP. Después, desarrollarás un job en streaming de Dataflow con el que leerás esos mensajes de Pub/sub, los mandarás a tu microservicio de inferencia para que haga las predicciones y enviarás los resultados a un nuevo tópico de Pub/Sub.\n","\n","A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n","\n","![streaming_diagram](https://drive.google.com/uc?export=view&id=1TEBPPc9ZF09IM5iGq9FwGAx9PVzAYNPg)"]},{"cell_type":"markdown","metadata":{"id":"Yw3oY8n5GjuU"},"source":["Primero, creamos el publisher que será el encargado de recoger los mensajes de Twitch y enviarlos a Pub/Sub. Esto os lo doy yo desarrollado, sólo tendréis que desplegarlo en una Cloud Function."]},{"cell_type":"code","metadata":{"id":"HH8WcRKrKwKy","executionInfo":{"status":"ok","timestamp":1657443241390,"user_tz":-120,"elapsed":219,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"source":["%mkdir -p /content/streaming/publisher"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"75whqxX7K54B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443241769,"user_tz":-120,"elapsed":3,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"7982d5d9-9740-4a6a-a9ec-c0dae2a655bf"},"source":["# Execute after restart\n","%cd /content/streaming/publisher"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/streaming/publisher\n"]}]},{"cell_type":"code","metadata":{"id":"jyOecviAK8QO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443243755,"user_tz":-120,"elapsed":284,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"b90b4d71-740d-4a22-bde9-a3f393728187"},"source":["%%writefile requirements.txt\n","\n","twitchio==1.2.3\n","loguru==0.5.3\n","google-cloud-pubsub==2.1.0"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting requirements.txt\n"]}]},{"cell_type":"code","source":["TOPIC_NAME = 'twitch'\n","PROJECT_ID"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"WNXYbgxLCAmI","executionInfo":{"status":"ok","timestamp":1657443244671,"user_tz":-120,"elapsed":6,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"f3677872-4e05-4462-cf63-bab2d3acac72"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'twitch-practice-355807'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"z-pnzR-jMkZ4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443246645,"user_tz":-120,"elapsed":294,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"73c2adfe-7838-41a4-8714-d257040d581e"},"source":["%%writefile main.py\n","\n","import os  # for importing env vars for the bot to use\n","import sys\n","import json\n","import time\n","\n","from twitchio.ext import commands\n","from google.cloud import pubsub_v1\n","from loguru import logger\n","\n","PROJECT_ID = os.getenv(\"PROJECT_ID\")\n","TOPIC_NAME = os.getenv(\"TOPIC_NAME\")\n","\n","TOPIC_PATH = f\"projects/{PROJECT_ID}/topics/{TOPIC_NAME}\"\n","\n","publisher = pubsub_v1.PublisherClient()\n","\n","class Bot(commands.Bot):\n","\n","    def __init__(self, irc_token='...', client_id='...', nick='...', prefix=\"!\", initial_channels=['...'], debug=True):\n","        super().__init__(irc_token=irc_token, client_id=client_id, nick=nick, prefix='!',\n","                         initial_channels=initial_channels)\n","        self.debug = debug\n","\n","    # Events don't need decorators when subclassed\n","    async def event_ready(self):\n","        logger.info('Ready')\n","\n","    async def event_message(self, message):\n","        logger.info(message.content)\n","        publisher.publish(TOPIC_PATH, str.encode(message.content))\n","\n","def main(request):\n","\n","    topic_name = f\"projects/{PROJECT_ID}/topics/{TOPIC_NAME}\"\n","    # publisher.create_topic(topic_name)\n","\n","    request_json = request.get_json(silent=True)\n","\n","    logger.info(\"Starting listener...\")\n","    if \"debug\" in request_json and isinstance(request_json[\"debug\"], bool):\n","        logger.info(f\"Debug mode: {request_json['debug']}\")\n","        bot = Bot(\n","          # set up the bot\n","          irc_token=\"oauth:xl5cpf8qe8tl1d03dppymchi6r04iz\",\n","          client_id=\"ciliqxi534iwg4pfqj7swl1jmkt23y\",\n","          nick=\"franalgaba\",\n","          prefix=\"!\",\n","          initial_channels=[\"franalgaba\"],\n","          debug=request_json['debug'])\n","    else:\n","        bot = Bot(\n","          # set up the bot\n","          irc_token=\"oauth:xl5cpf8qe8tl1d03dppymchi6r04iz\",\n","          client_id=\"ciliqxi534iwg4pfqj7swl1jmkt23y\",\n","          nick=\"franalgaba\",\n","          prefix=\"!\",\n","          initial_channels=[\"franalgaba\"])\n","\n","    bot.run()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main.py\n"]}]},{"cell_type":"code","source":["! pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cy7_BBw3UCfF","executionInfo":{"status":"ok","timestamp":1657443252567,"user_tz":-120,"elapsed":3054,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"e277f39a-62ae-4cb3-c1f6-48aa608f26c5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: twitchio==1.2.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.2.3)\n","Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.5.3)\n","Requirement already satisfied: google-cloud-pubsub==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.1.0)\n","Requirement already satisfied: aiohttp>=3.3 in /usr/local/lib/python3.7/dist-packages (from twitchio==1.2.3->-r requirements.txt (line 2)) (3.8.1)\n","Requirement already satisfied: websockets>=6.0 in /usr/local/lib/python3.7/dist-packages (from twitchio==1.2.3->-r requirements.txt (line 2)) (10.3)\n","Requirement already satisfied: async-timeout>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from twitchio==1.2.3->-r requirements.txt (line 2)) (4.0.2)\n","Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.12.4)\n","Requirement already satisfied: libcst>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.4.6)\n","Requirement already satisfied: proto-plus>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.20.6)\n","Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.31.6)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (2.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (1.7.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (21.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (4.1.1)\n","Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.20.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (57.4.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (21.3)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2.25.0)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.56.2)\n","Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.35.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2022.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.46.3)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (4.2.4)\n","Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.3.10->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.7.1)\n","Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.3.10->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.0.9)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from typing-inspect>=0.4.0->libcst>=0.3.10->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.4.3)\n"]}]},{"cell_type":"code","source":["from main import main\n","\n","class ResponseMock():\n","\n","  def __init__(self, data):\n","    self.data = data\n","\n","  def get_json(self, silent):\n","    return self.data\n","\n","main(ResponseMock({\"filter\": \"spain\", \"debug\": True}))"],"metadata":{"id":"k04ZnBpsTu6f","executionInfo":{"status":"ok","timestamp":1657443642632,"user_tz":-120,"elapsed":318,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDrQjJbsVO-4","executionInfo":{"status":"ok","timestamp":1657444771021,"user_tz":-120,"elapsed":221,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"source":["# In case user service error...\n","! gcloud iam service-accounts add-iam-policy-binding <project_id>@appspot.gserviceaccount.com --member=user:<mail> --role=roles/iam.serviceAccountUser"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7x_BX3FCG3mD"},"source":["Para lanzar vuestra Cloud Function, que recoja y mande mensajes solo tenéis que ejecutar el comando siguiente (haced los pasos vistos en clase para desplegar el servicio):"]},{"cell_type":"code","metadata":{"id":"PcemNpnsKxFy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443401178,"user_tz":-120,"elapsed":3379,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"debbac6c-0e48-4d5a-97a4-8e9b292a4457"},"source":["! curl -X POST https://europe-west1-twitch-practice-355807.cloudfunctions.net/twitch-publisher -H \"Content-Type:application/json\"  -d '{\"debug\": false}'"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: could not handle the request\n"]}]},{"cell_type":"markdown","metadata":{"id":"Yr3SmFrUHCvN"},"source":["## Entregable 1 (3 puntos)\n","\n","En este entregable desarrollarás un pipeline de inferencia en streaming usando Apache Beam para ejecutar en Dataflow un job en streaming que llamará a vuestro microservicio para realizar inferencias."]},{"cell_type":"code","metadata":{"id":"IwlDH0KFbn1q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443722867,"user_tz":-120,"elapsed":266,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"744d5baa-27d2-4aae-a65a-1cae5a640d6a"},"source":["%mkdir /content/streaming/subscriber"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/streaming/subscriber’: File exists\n"]}]},{"cell_type":"code","metadata":{"id":"DLNtrkgybtHE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443723212,"user_tz":-120,"elapsed":3,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"ac3c98a5-6cd8-4fc8-9900-53cd96d4dec4"},"source":["%cd /content/streaming/subscriber"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/streaming/subscriber\n"]}]},{"cell_type":"code","metadata":{"id":"XtBrcF2zbz04","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443724937,"user_tz":-120,"elapsed":221,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"ddc254f6-2d54-47a2-9d32-f98ef54c5e03"},"source":["%%writefile requirements.txt\n","\n","apache-beam[gcp]==2.24.0\n","fsspec==0.8.4\n","gcsfs==0.7.1\n","loguru==0.5.3"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting requirements.txt\n"]}]},{"cell_type":"code","metadata":{"id":"6RN9KNMob1jY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443730175,"user_tz":-120,"elapsed":3804,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"c93a3f02-83ec-4ffb-f2d8-398d7d1e4744"},"source":["! pip install -r requirements.txt"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: apache-beam[gcp]==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.24.0)\n","Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.8.4)\n","Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.7.1)\n","Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.5.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (3.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (4.4.2)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (0.4.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (2.25.0)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (1.35.0)\n","Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.18.2)\n","Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.7.4.3)\n","Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.46.3)\n","Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.4)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.12.3)\n","Requirement already satisfied: pyarrow<0.18.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.1)\n","Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.3.1.1)\n","Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.0.0)\n","Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.5)\n","Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.9.2.1)\n","Requirement already satisfied: fastavro<0.24,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.23.6)\n","Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2022.1)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7)\n","Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.7.0)\n","Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.20.0)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.0)\n","Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.2)\n","Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.2)\n","Requirement already satisfied: cachetools<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.1.1)\n","Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.5.31)\n","Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.3)\n","Requirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.8.0)\n","Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.2)\n","Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.2)\n","Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.21.0)\n","Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.2)\n","Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.2)\n","Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.16.3)\n","Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.2.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.3)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (57.4.0)\n","Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.1)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.31.6)\n","Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.12.4)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.56.2)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (21.3)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.6.2)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.7/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (5.9.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.8)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.7.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (21.4.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (2.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.2.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.2.0)\n"]}]},{"cell_type":"code","metadata":{"id":"JMeJPR90b3AN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443734426,"user_tz":-120,"elapsed":235,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"9dee5ee6-9c79-4752-92fd-28474a5e976b"},"source":["%%writefile predict.py\n","\n","from __future__ import absolute_import\n","from __future__ import print_function\n","\n","import argparse\n","import requests\n","import json\n","import sys\n","\n","import apache_beam as beam\n","import apache_beam.transforms.window as window\n","from apache_beam.options.pipeline_options import (\n","    GoogleCloudOptions,\n","    StandardOptions,\n","    PipelineOptions,\n","    SetupOptions,\n",")\n","from loguru import logger\n","\n","\n","class Predict(beam.DoFn):\n","    def __init__(self, predict_server) -> None:\n","        self.url = predict_server\n","\n","    def _predict(self, text) -> str:\n","        payload = {\"text\": text}\n","        headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n","        try:\n","            response = requests.post(\n","                self.url, data=json.dumps(payload), headers=headers\n","            )\n","            response = json.loads(response.text)\n","        except Exception:\n","            response = {\"label\": \"undefined\", \"score\": 0, \"elapsed_time\": 0}\n","\n","        return response\n","\n","    def process(self, element, window=beam.DoFn.WindowParam):\n","        logger.info(f\"Text to predict: {element}\")\n","        result = self._predict(element)\n","        result[\"text\"] = element\n","        yield json.dumps(result)\n","\n","\n","def run(predict_server, source, sink, beam_options=None):\n","    with beam.Pipeline(options=beam_options) as p:\n","        _ = (\n","            p\n","            | \"Read data from PubSub\" >> source\n","            | \"decode\" >> beam.Map(lambda x: x.decode(\"utf-8\"))\n","            | \"window\" >> beam.WindowInto(window.FixedWindows(15))\n","            | \"Predict\" >> beam.ParDo(Predict(predict_server))\n","            | \"encode\" >> beam.Map(lambda x: x.encode(\"utf-8\")).with_output_types(bytes)\n","            | \"Write predictions\" >> sink\n","        )\n","\n","\n","if __name__ == \"__main__\":\n","    \"\"\"Main function\"\"\"\n","    parser = argparse.ArgumentParser(\n","        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n","    )\n","\n","    parser.add_argument(\n","        \"--inputs_topic\",\n","        dest=\"inputs_topic\",\n","        required=True,\n","        help=\"Directory for temporary files and preprocessed datasets to. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--outputs_topic\",\n","        dest=\"outputs_topic\",\n","        required=True,\n","        help=\"Directory for temporary files and preprocessed datasets to. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--predict_server\",\n","        dest=\"predict_server\",\n","        required=True,\n","        help=\"Directory for temporary files and preprocessed datasets to. \"\n","        \"This can be a Google Cloud Storage path.\",\n","    )\n","\n","    args, pipeline_args = parser.parse_known_args()\n","    logger.info(args)\n","    beam_options = PipelineOptions(pipeline_args)\n","    beam_options.view_as(SetupOptions).save_main_session = True\n","    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n","\n","    project = beam_options.view_as(GoogleCloudOptions).project\n","\n","    if not project:\n","        parser.print_usage()\n","        print(\"error: argument --project is required for streaming\")\n","        sys.exit(1)\n","\n","    beam_options.view_as(StandardOptions).streaming = True\n","\n","    source = beam.io.ReadFromPubSub(\n","        topic=\"projects/{}/topics/{}\".format(project, args.inputs_topic)\n","    ).with_output_types(bytes)\n","\n","    sink = beam.io.WriteToPubSub(\n","        topic=\"projects/{}/topics/{}\".format(project, args.outputs_topic)\n","    )\n","\n","    run(args.predict_server, source, sink, beam_options)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing predict.py\n"]}]},{"cell_type":"code","metadata":{"id":"jT_Qmb16hgEq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657443739163,"user_tz":-120,"elapsed":240,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"619e6aae-f971-4caa-a19a-51eeacd4ef58"},"source":["%%writefile setup.py\n","\n","import setuptools\n","\n","REQUIRED_PACKAGES = [\n","    \"apache-beam[gcp]==2.24.0\",\n","    \"fsspec==0.8.4\",\n","    \"gcsfs==0.7.1\",\n","    \"loguru==0.5.3\",\n","]\n","\n","setuptools.setup(\n","    name=\"twitchstreaming\",\n","    version=\"0.0.1\",\n","    install_requires=REQUIRED_PACKAGES,\n","    packages=setuptools.find_packages(),\n","    include_package_data=True,\n","    description=\"Twitch Troll Detection\",\n",")"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing setup.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"-RdyxvdVHmnZ"},"source":["### Validación inferencia en streaming\n","\n","Con el comando mostrado a continuación se genera un job en streaming de Dataflow. Antes de ejecutarlo, deberás crear dos topicos en Pub/Sub, `twitch-chat` donde se recibirán los mensajes de twitch, y `twitch-chat-predictions` donde se mandarán las predicciones generadas por vuestro microservicio.\n","\n","**Importante**: no te olvides de modificar la URL de tu microservicio de inferencia."]},{"cell_type":"code","metadata":{"id":"9j4vzFn2wUPk","executionInfo":{"status":"ok","timestamp":1657444337821,"user_tz":-120,"elapsed":256,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}}},"source":["GCP_WORK_DIR = 'gs://' + BUCKET_NAME\n","GCP_REGION = \"europe-west1\"\n","\n","#GCP_WORK_DIR = 'gs://final-practice-test-execution'\n","#GCP_REGION = 'us-east1'"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5ftKoXAhiJ7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657444310914,"user_tz":-120,"elapsed":9340,"user":{"displayName":"Irena Vent","userId":"07653679291647584513"}},"outputId":"eae55c22-8ac6-4d52-c5b1-389767c784e7"},"source":["! python3 predict.py \\\n","--project $PROJECT_ID \\\n","--region $GCP_REGION \\\n","--runner DataflowRunner \\\n","--temp_location $GCP_WORK_DIR/beam-temp \\\n","--setup_file ./setup.py \\\n","--inputs_topic twitch \\\n","--outputs_topic twitch-chat-predictions \\\n","--predict_server https://troll-detection-online-service-73bjy6adhq-ew.a.run.app/api/model/predict \\"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m2022-07-10 09:11:42.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mNamespace(inputs_topic='twitch', outputs_topic='twitch-chat-predictions', predict_server='https://troll-detection-online-service-73bjy6adhq-ew.a.run.app/api/model/predict')\u001b[0m\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","warning: check: missing required meta-data: url\n","\n","warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/subprocess.py\", line 490, in run\n","    stdout, stderr = process.communicate(input, timeout=timeout)\n","  File \"/usr/lib/python3.7/subprocess.py\", line 951, in communicate\n","    stdout = self.stdout.read()\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"predict.py\", line 111, in <module>\n","    run(args.predict_server, source, sink, beam_options)\n","  File \"predict.py\", line 54, in run\n","    | \"Write predictions\" >> sink\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 555, in __exit__\n","    self.result = self.run()\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 534, in run\n","    return self.runner.run_pipeline(self, self._options)\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py\", line 479, in run_pipeline\n","    artifacts=environments.python_sdk_dependencies(options)))\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/transforms/environments.py\", line 613, in python_sdk_dependencies\n","    staged_name in stager.Stager.create_job_resources(options, tmp_dir))\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 235, in create_job_resources\n","    resources.extend(Stager._create_beam_sdk(sdk_remote_location, temp_dir))\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 625, in _create_beam_sdk\n","    sdk_local_file = Stager._download_pypi_sdk_package(temp_dir)\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 731, in _download_pypi_sdk_package\n","    processes.check_output(cmd_args)\n","  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/utils/processes.py\", line 91, in check_output\n","    out = subprocess.check_output(*args, **kwargs)\n","  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n","    output=stdout, stderr=stderr)\n","  File \"/usr/lib/python3.7/subprocess.py\", line 866, in __exit__\n","    self._wait(timeout=self._sigint_wait_secs)\n","  File \"/usr/lib/python3.7/subprocess.py\", line 1647, in _wait\n","    time.sleep(delay)\n","KeyboardInterrupt\n"]}]}]}